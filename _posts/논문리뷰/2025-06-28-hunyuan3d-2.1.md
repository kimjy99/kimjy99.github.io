---
title: "[논문리뷰] Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material"
last_modified_at: 2025-06-28
categories:
  - 논문리뷰
tags:
  - Diffusion
  - Mesh Generation
  - 3D Vision
excerpt: "Hunyuan3D 2.1 논문 리뷰"
use_math: true
classes: wide
---

> arXiv 2025. [[Paper](https://arxiv.org/abs/2506.15442)] [[Github](https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1)]  
> Tencent Hunyuan3D Team  
> 18 Jun 2025  

<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-fig1.webp" | relative_url}}' width="100%"></center>

> [Hunyuan3D 2.0](https://kimjy99.github.io/논문리뷰/hunyuan3d-2.0)  

## Introduction
본 논문은 단일 이미지 입력으로부터 텍스처 메쉬를 생성하는 포괄적인 3D 에셋 생성 시스템인 **Hunyuan3D 2.1**을 소개한다. 이 시스템은 두 가지 foundation model을 기반으로 한다. 

1. **Hunyuan3D-DiT**: Flow-based diffusion 아키텍처와 고충실도 메쉬 오토인코더인 **Hunyuan3D-ShapeVAE**를 결합한 shape 생성 모델
2. **Hunyuan3D-Paint**: PBR material 생성을 위한 메쉬로 컨디셔닝된 멀티뷰 diffusion model

Hunyuan3D-ShapeVAE는 메쉬 표면 중요도 샘플링을 통해 날카로운 모서리를 향상시키고, 가변적인 토큰 길이를 통해 복잡한 기하학적 디테일을 개선하였다. Hunyuan3D-DiT는 최신 flow matching 모델을 계승하여 scalable하고 유연한 diffusion model을 구축하였다. 

텍스처 합성을 위해 Hunyuan3D-Paint는 메쉬에 대한 albedo, metallic, roughness 맵을 생성하는 멀티뷰 PBR diffusion을 도입하였다. 특히, Hunyuan3D-Paint는 albedo map과 metallic-roughness (MR) map을 정렬하는 spatial-aligned multi-attention 모듈, 시점 간 일관성을 향상시키는 3D-aware RoPE, 그리고 다양한 조명 조건에 robust한 무광 albedo map을 생성하는 조명 불변 (illumination-invariant) 학습 전략을 통합하였다. 

## Data Processing
저자들은 ShapeNet, ModelNet40, Thingi10K, Objaverse에서 약 10만 개의 3D 데이터를 수집하여 shape 생성 모델 학습에 사용했다. 또한, 엄격한 큐레이션 프로토콜에 따라 Objaverse-XL을 필터링하여 3D 데이터 약 7만 개를 얻고, 이를 텍스처 합성 모델 학습에 사용했다.

### 1. Data preprocessing for shape generation
##### Normalization
먼저, 각 3D 물체에 대해 axis-aligned bounding box (AABB)를 계산한다. 균일한 scaling을 적용하여 물체를 원점을 중심으로 하는 단위 정육면체에 맞추면서 종횡비를 유지하면서 전체 데이터셋에서 일관된 scale을 유지한다. 이러한 공간 정규화는 신경망이 일관된 기하학적 패턴을 학습하는 데 특히 중요한데, 학습된 feature에 영향을 줄 수 있는 지나친 크기 변화를 제거하기 때문이다. 

포인트 클라우드 데이터의 경우, 모든 포인트의 좌표에서 중심점의 좌표를 뺀 다음, 중심으로부터 최대 유클리드 거리로 모든 포인트를 scaling한다. 이 방식은 모든 물체가 정규화된 공간에서 원래의 기하학적 관계를 유지하면서 거의 동일한 부피를 차지하도록 보장한다.

##### Watertight
IGL 라이브러리는 결함 있는 형상으로부터 SDF를 구성하여 빈틈 없는 표면을 생성한다. 입력 메쉬를 포함하는 균일한 3D query 그리드를 초기화한다. 각 query 포인트 $\textbf{q} \in Q_g$에 대해 IGL은 다음을 계산한다.

$$
\begin{equation}
\textrm{SDF}(\textbf{q}) = \textrm{distance_to_mesh} (\textbf{q}, V, F) \cdot \textrm{sign} (\omega (\textbf{q}))
\end{equation}
$$

($V$는 vertex 집합, $F$는 face 집합, $\omega (\textbf{q})$는 일반화된 winding number)

부호 일관성은 IGL의 winding number 계산을 사용하여 강화되며, $\omega$가 1에 가까우면 내부의 점, 0에 가까우면 외부의 점을 나타낸다. 따라서 내부 분류를 위해 $\omega > 0.5$의 threshold를 설정하여 self-intersection 근처의 모호한 부호를 해결한다. Zero-level isosurface에서 marching cube를 통해 빈틈 없는 메쉬가 추출된다. 출력 $$(V_\textrm{iso}, F_\textrm{iso})$$는 경계 불연속성이 없는 위상적으로 닫힌 표면을 형성한다.

##### SDF Sampling
본 논문에서는 SDF 생성을 3D 형상 표현의 핵심 수학적 프레임워크로 활용하였다. 이를 위해 두 가지 방식으로 query 포인트를 무작위로 선택하는 전략을 사용한다. 

1. 형상 표면에 가까운 포인트
2. $[-1, 1]^3$ 공간 전체에 고르게 분포된 포인트

IGL 컴퓨팅 라이브러리를 사용하여 이러한 포인트들의 SDF 값을 계산한다. 표면 근처의 포인트에서 얻은 SDF 값은 형상 표면의 복잡한 디테일을 포착하는 데 매우 중요하다. 이를 통해 모델은 형상 구조의 미묘한 변화를 정확하게 표현할 수 있다. Uniform sampling된 포인트에서 얻은 SDF 값은 모델이 3D 형상의 전반적인 구조와 형태를 더 폭넓게 이해할 수 있도록 한다. 이러한 이중 샘플링 방식을 통해 모델은 형상의 세부적인 측면과 일반적인 측면 모두를 포괄적으로 이해할 수 있다.

##### Surface Sampling
본 논문의 하이브리드 샘플링 전략은 uniform sampling과 feature-aware sampling의 장점을 결합하여 완전한 형상 정보를 포착한다. Uniform sampling은 표면 전체에 걸쳐 균일한 적용 범위를 보장하여 최종 포인트 세트의 약 50%를 형성한다. 나머지 50%는 로컬 표면 도함수 기반의 중요도 샘플링을 통해 곡률이 높은 feature 근처에 전략적으로 배치된다. 샘플링 밀도는 기하학적 복잡성에 따라 자동으로 조정되어 복잡한 디테일이 있는 영역에서는 포인트 밀도를 높이는 동시에 단순한 영역에서는 포인트 밀도를 낮춘다. 

이러한 균형 잡힌 접근 방식은 평면 영역을 불필요하게 dense하게 샘플링하지 않고도 날카로운 모서리, 꼭짓점, 기타 feature가 적절하게 표현되도록 하여 포인트 세트의 품질과 효율성을 최적화한다.

##### Condition Render
Shape 생성 학습을 위한 조건 이미지를 렌더링하기 위해, 원점을 중심으로 하는 구면에 균일하게 분포된 150대의 카메라를 Hammersley sequence 알고리즘을 사용하여 랜덤 offset $\delta \in [0, 1)^2$로 샘플링한다. FOV는 $$\theta_\textrm{aug} \sim \mathcal{U}(10^\circ, 70^\circ)$$, 카메라 반경은 $$r_\textrm{aug} \in [1.51, 9.94]$$에서 샘플링된다. 

### 2. Data preprocessing for texture synthesis
텍스처 합성을 위한 데이터셋은 Objaverse와 Objaverse-XL에서 필터링된 7만 개 이상의 사람이 주석을 단 고품질 데이터로 구성된다. 

1. 각 3D 물체에 대해 4가지 고도 각도 ($−20^\circ$, $0^\circ$, $20^\circ$, 랜덤 각도)에서 데이터를 렌더링. (랜덤 각도는 $[-30^\circ, 70^\circ]$에서 샘플링)
2. 각 고도 각도에서 방위각에 따라 균일하게 분포된 24개의 뷰를 선택하여 해당 albedo, metallic, roughness map과 512$\times$512 해상도의 HDR/point-light 이미지를 생성 (point light 30%, HDR map 70%)

## Training
### 1. Hunyuan3D-Shape
Shape 생성 모델은 은 latent diffusion model을 아키텍처로 사용하며, 두 부분으로 구성된다.

1. **Hunyuan3D-ShapeVAE**: 폴리곤 메쉬로 표현된 3D 에셋을 latent space의 연속적인 토큰 시퀀스로 압축하는 오토인코더
2. **Hunyuan3D-DiT**: 사용자가 제공한 이미지에서 물체 토큰 시퀀스를 예측하기 위해 ShapeVAE의 latent space에서 학습된 flow-based diffusion model

#### 1.1 Hunyuan3D-ShapeVAE
> [Hunyuan3D 2.0](https://kimjy99.github.io/논문리뷰/hunyuan3d-2.0)과 동일

Hunyuan3D-ShapeVAE는 [3DShape2VecSet](https://arxiv.org/abs/2301.11445)에서 제안한 3D 형상에 대한 컴팩트 신경망 표현인 벡터 세트를 사용한다. [Michelangelo](https://arxiv.org/abs/2306.17115)를 따라, 형상 압축 및 디코딩을 위해 variational encoder-decoder transformer를 사용한다. 또한, 3D 형상 표면에서 샘플링된 포인트 클라우드의 3D 좌표와 normal 벡터를 인코더의 입력으로 선택하고, 디코더가 3D 형상의 SDF를 예측하도록 한다. 이 SDF는 marching cube 알고리즘을 통해 삼각형 메쉬로 디코딩될 수 있다. 

##### 인코더
1. 입력 메쉬에 대해 균일하게 샘플링된 표면 포인트 클라우드 $P_u \in \mathbb{R}^{M \times 3}$와 중요도 샘플링된 표면 포인트 클라우드 $P_i \in \mathbb{R}^{N \times 3}$를 수집한다. 
2. 포인트 쿼리를 얻기 위해, $P_u$와 $P_i$에 각각 Farthest Point Sampling (FPS)을 적용하여 $Q_u \in \mathbb{R}^{M^\prime \times 3}$와 $Q_i \in \mathbb{R}^{N^\prime \times 3}$를 얻는다.
3. $P_u$와 $P_i$를 concat하여 $P \in \mathbb{R}^{(M + N) \times 3}$을 얻고, $Q_u$와 $Q_i$를 concat하여 $Q \in \mathbb{R}^{(M^\prime + N^\prime) \times 3}$을 얻는다.
4. $P$와 $Q$에 Fourier positional encoding과 linear projection을 각각 순차적으로 적용하여 $X_p \in \mathbb{R}^{(M + N) \times d}$와 $X_q \in \mathbb{R}^{(M^\prime + N^\prime) \times d}$로 인코딩한다. 
5. $X_p$를 key와 value로, $X_q$를 query로 하여 cross-attention을 수행한다.
6. 8개의 self-attention layer를 통과시켜 $H_s \in \mathbb{R}^{(M^\prime + N^\prime) \times d}$로 feature 표현을 향상시킨다.
7. VAE 디자인을 사용하기 때문에, $H_s$에 추가 linear projection을 적용하여 latent shape embedding $Z_s$의 평균 $E(Z_s) \in \mathbb{R}^{(M^\prime + N^\prime) \times d_0}$와 분산 $\textrm{Var}(Z_s) \in \mathbb{R}^{(M^\prime + N^\prime) \times d_0}$을 예측한다.

##### 디코더
디코더 $$\mathcal{D}_s$$는 인코더의 latent shape embedding $Z_s$로부터 3D neural field를 재구성한다. 

1. Projection layer로 latent shape embedding의 차원 $d_0$에서 transformer의 차원 $d$로 차원을 다시 변환한다.
2. 여러 self-attention layer를 통과시킨다.
3. 3D 그리드 $Q_g \in \mathbb{R}^{(H \times W \times D) \times 3}$을 query로 사용한 cross-attention을 통해 3D neural field $F_g \in \mathbb{R}^{(F_n \times W \times D) \times d}$를 얻는다.
4. Neural field에 대한 또 다른 linear projection을 사용하여 SDF $$F_\textrm{sdf} \in \mathbb{R}^{(F_0 \times W \times D) \times 1}$$을 얻는다.

얻은 SDF는 marching cube 알고리즘을 사용하여 삼각형 메쉬로 디코딩할 수 있다.

##### 학습 전략
모델 학습에 여러 loss를 사용한다. 

1. **Reconstruction loss**: 예측된 SDF $$\mathcal{D}_s (x \vert Z_s)$$와 GT $\textrm{SDF}(x)$ 사이의 MSE loss
2. **KL-divergence loss** $$\mathcal{L}_\textrm{KL}$$: Latent space를 컴팩트하고 연속적으로 만들어 diffusion model의 학습을 용이하게 함

완전한 SDF에 dense한 계산이 필요하기 때문에 인해, reconstruction loss는 공간 및 형상 표면에서 무작위로 샘플링된 포인트에 대한 loss의 기대값으로 계산된다. 전체 학습 손실 $$\mathcal{L}_r$$은 다음과 같다.

$$
\begin{equation}
\mathcal{L}_r = \mathbb{E}_{x \in \mathbb{R}^3} [\textrm{MSE} (\mathcal{D}_s (x \vert Z_s), \textrm{SDF}(x))] + \gamma \mathcal{L}_\textrm{KL}
\end{equation}
$$

학습 과정에서는 모델 수렴 속도를 높이기 위해 다중 해상도 전략을 활용한다. 이 전략에서는 latent 토큰 시퀀스의 길이가 미리 정의된 집합에서 무작위로 샘플링된다. 시퀀스가 ​​짧을수록 계산 비용이 감소하고, 시퀀스가 ​​길수록 재구성 품질이 향상된다. 가장 긴 시퀀스 길이는 3072로, 세밀하고 선명한 디테일을 가진 고해상도 생성을 지원할 수 있다.

#### 1.2 Hunyuan3D-DiT
Hunyuan3D-DiT는 주어진 이미지 프롬프트에 따라 고정확도, 고해상도의 3D shape을 생성하는 것을 목표로 하는 flow-based diffusion model이다.

##### 조건 인코더
저자들은 디테일한 이미지 feature를 포착하기 위해 518$\times$518 이미지 크기를 가진 대형 이미지 인코더인 [DINOv2 Giant](https://kimjy99.github.io/논문리뷰/dinov2)를 사용했다. 또한, 입력 이미지에서 배경을 제거하고, 물체의 크기를 조정한 후 중앙에 배치하고 배경을 흰색으로 채웠다.

##### DiT block
<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-fig2.webp" | relative_url}}' width="100%"></center>
<br>
위 그림과 같이 transformer 구조를 채택했으며, 21개의 transformer layer를 쌓아 latent code를 학습했다. 각 transformer layer에서 차원 concat을 통해 latent code의 skip connection을 도입했다. Cross-attention layer를 사용하여 이미지 조건을 latent code로 projection시키고, MOE layer를 사용하여 latent code의 표현 학습을 향상시켰다.

<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-fig3.webp" | relative_url}}' width="57%"></center>

##### 학습 & inference
모델 학습에는 flow matching 목적 함수를 활용한다. 구체적으로, flow matching은 먼저 Gaussian 분포와 데이터 분포 사이의 확률 밀도 경로를 정의한 후, 샘플 $x_t$가 데이터 $x_1$ 방향으로 이동하는 속도장 $u_t$를 예측하도록 모델을 학습시킨다. 본 논문에서는 다음과 같은 affine 경로를 채택하였다. 

$$
\begin{equation}
x_t = (1 − t) \times x_0 + t \times x_1, \quad u_t = x_1 − x_0
\end{equation}
$$

따라서 학습 loss는 다음과 같다.

$$
\begin{equation}
\mathcal{L} = \mathbb{E}_{t \sim \mathcal{U}(0,1), x_0, x_1} [\| u_\theta (x_t, c, t) - u_t \|_2^2 ]
\end{equation}
$$

Inference 시에는 먼저 시작점 $x_0 \sim \mathcal{N}(0, 1)$을 무작위로 샘플링하고 1차 오일러 ODE solver를 사용하여 diffusion model $x_1$을 계산한다.

### 2. Hunyuan3D-Paint
<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-fig4.webp" | relative_url}}' width="100%"></center>
<br>
기존의 색상 텍스처는 더 이상 사실적인 3D 애셋 생성에 대한 요구를 충족하기에 충분하지 않다. 따라서 기존 RGB 텍스처 맵을 뛰어넘는 PBR material 텍스처 합성 프레임워크를 도입하였다. BRDF 모델을 준수하고 여러 시점에서 albedo, metallic, roughness map을 동시에 출력하여 생성된 3D 애셋의 표면 반사율 특성을 정확하게 표현하고 기하학적 미세 표면의 분포를 정밀하게 시뮬레이션하여 더욱 사실적이고 세부적인 렌더링 효과를 구현하였다. 또한, 3D-aware RoPE를 도입하여 공간 정보를 주입하여 시점 간 일관성을 크게 향상시키고 매끄러운 텍스처링을 구현하였다.

##### 기본 아키텍처
[Hunyuan3D-2](https://kimjy99.github.io/논문리뷰/hunyuan3d-2.0)의 멀티뷰 텍스처 생성 아키텍처를 기반으로, 그림 4의 왼쪽에 보이는 것과 같은 새로운 material 생성 프레임워크를 소개합니다. 이 프레임워크는 Disney Principled BRDF 모델을 구현하여 고품질 PBR material map을 생성한다. ReferenceNet의 레퍼런스 이미지 feature 주입 메커니즘을 그대로 유지하면서, normal map, latent noise, CCM (canonical coordinate map)을 모두 concat한다.

##### Spatial-Aligned Multi-Attention Module
저자들은 material 이미지 압축을 위해 사전 학습된 VAE를 사용하는 동시에, material 생성을 위해 병렬 dual-branch UNet 아키텍처를 구현하였다. Albedo map과 metallic-roughness (MR) map 모두에 대해 self-attention, multi-view attention, reference attention으로 구성된 병렬 multi-attention 모듈을 구현하였다. Albedo/MR map과 레퍼런스 이미지 간의 물리적 관계를 모델링하고 MR map과 albedo map 간의 공간적 정렬을 위해, albedo reference attention 모듈에서 계산된 출력을 MR branch로 직접 전파한다.

##### 3D-Aware RoPE
인접 시점 간 로컬 불일치로 인해 발생하는 텍스처 이음새와 고스팅(ghosting) 현상을 해결하기 위해, [RomanTex](https://arxiv.org/abs/2503.19011)의 3D-aware RoPE를 multi-view attention block에 도입하여 시점 간의 일관성을 향상시킨다. 구체적으로, 3D 좌표 볼륨을 다운샘플링하여 UNet 계층 레벨에 맞춰 다중 해상도의 3D 좌표 인코딩을 구성한다. 이러한 인코딩은 해당 hidden state와 더해져 시점 간의 상호작용을 3D 공간에 통합하고 멀티뷰 일관성을 강화한다.

##### Illumination-Invariant Training Strategy
저자들은 빛과 그림자가 없는 albedo map과 정확한 MR map을 생성하기 위해, 조명 불변 (illumination-invariant) 학습 전략을 설계하였다. 구체적으로, 일관성 loss는 서로 다른 조명 조건에서 렌더링된 동일한 물체의 레퍼런스 이미지를 포함하는 두 세트의 학습 샘플을 채택하여 계산된다.

## Evaluation
- 구현 디테일
  - base model: Stable Diffusion 2 v-model의 [ZSNR](https://arxiv.org/abs/2305.08891) checkpoint
  - optimizer: AdmaW
  - learning rate: $5 \times 10^{-5}$ (2000 warm-up step)
  - 학습 시간: 180 GPU-day

### 1. 3D Shape Generation
다음은 shape 생성 모델들과 비교한 결과이다.

<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-fig5.webp" | relative_url}}' width="82%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-table1.webp" | relative_url}}' width="63%"></center>

### 2. Texture Map Synthesis
다음은 텍스처 합성 모델들과 비교한 결과이다.

<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-fig6.webp" | relative_url}}' width="82%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-table2.webp" | relative_url}}' width="63%"></center>
<br>
다음은 image-to-3D 모델들과 비교한 결과이다.

<center><img src='{{"/assets/img/hunyuan3d-2.1/hunyuan3d-2.1-fig7.webp" | relative_url}}' width="100%"></center>