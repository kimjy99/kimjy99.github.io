---
title: "[논문리뷰] PTI: Pivotal Tuning for Latent-based Editing of Real Images"
last_modified_at: 2023-04-22
categories:
  - 논문리뷰
tags:
  - Fine-Tuning
  - GAN Inversion
  - GAN
  - Image-to-Image Translation
  - Computer Vision
  - AI
excerpt: "PTI 논문 리뷰 (ACM TOG 2022)"
use_math: true
classes: wide
---

> ACM TOG 2022. [[Paper](https://arxiv.org/abs/2106.05744)] [[Github](https://github.com/danielroich/PTI)]  
> Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or  
> The Blavatnik School of Computer Science, Tel Aviv University  
> 10 Jun 2021  

<center><img src='{{"/assets/img/pti/pti-fig1.PNG" | relative_url}}' width="60%"></center>

## Introduction
최근 몇 년 동안 unconditional한 이미지 합성은 GAN의 등장으로 엄청난 발전을 이루었다. 본질적으로 GAN은 원하는 이미지셋의 도메인(또는 manifold)을 학습하고 동일한 분포에서 새 샘플을 생성한다. 특히 StyleGAN은 이 작업에 가장 많이 사용되는 선택지 중 하나이다. State-of-the-art의 시각적 충실도와 다양성을 달성했을 뿐만 아니라, 유기적으로 형성된 disentangle한 latent space로 인해 환상적인 편집 능력을 발휘한다. 이 속성을 사용하여 많은 방법이 학습된 manifold를 통과하여 얼굴 방향, 표정 또는 연령 변경과 같은 StylGAN의 latent space에 대한 현실적인 편집 능력을 보여준다. 

인상적이지만 이러한 편집은 generator의 latent space에서 엄격하게 수행되며 해당 영역을 벗어난 실제 이미지에는 적용할 수 없다. 따라서 실제 이미지를 편집하는 것은 latent 표현을 찾는 것에서 시작된다. GAN inversion이라고 하는 이 프로세스는 최근 상당한 관심을 끌었다. 초기 시도는 이미지를 StyleGAN의 기본 latent space인 $\mathcal{W}$로 invert시켰다. 그러나 [Image2stylegan](https://arxiv.org/abs/1904.03189)은 실제 이미지를 이 space로 invert하면 왜곡, 즉 주어진 이미지와 생성된 이미지 간의 불일치가 발생하여 정체성 손실 또는 부자연스러운 모양과 같은 아티팩트가 발생하는 것으로 나타났다. 따라서 현재 inversion 방법은 종종 $\mathcal{W}+$로 표시되는 확장된 latent space를 사용하며, 이는 보다 표현력이 풍부하고 훨씬 적은 왜곡을 유발한다. 

그러나 $\mathcal{W}+$의 코드를 사용하면 도메인 외부 이미지의 경우에도 잠재적으로 뛰어난 시각적 품질을 생성할 수 있지만 이러한 code는 generator의 학습된 도메인이 아니기 때문에 편집 가능성이 떨어진다. [e4e](https://arxiv.org/abs/2102.02766)는 이 충돌을 왜곡-편집 가능성 trade-off로 정의하고 code가 $\mathcal{W}$에 가까울수록 편집 가능성이 더 높다는 것을 보여준다. 실제로 최근 논문에서는 편집 가능한 $\mathcal{W}+$의 latent code를 선택하여 편집 가능성과 왜곡 간의 절충안을 제안한다.

본 논문에서는 왜곡-편집 가능성 trade-off를 완화하여 분포되지 않은 실제 이미지에 대한 설득력 있는 편집을 허용하는 새로운 접근 방식을 소개한다. 학습된 manifold에 입력 이미지를 project하는 대신 generator를 약간 변경하여 이미지를 포함하도록 manifold를 확대한다. 이 프로세스를 **Pivotal Tuning**이라고 한다. 이 튜닝은 다트를 쏜 다음 근접 타격을 보상하기 위해 보드 자체를 이동하는 것과 유사하다.

<center><img src='{{"/assets/img/pti/pti-fig2.PNG" | relative_url}}' width="100%"></center>
<br>
StyleGAN 학습은 비용이 많이 들고 generator가 전례 없는 시각적 품질을 달성하기 때문에 널리 사용되는 접근 방식은 generator를 고정된 상태로 유지하는 것이다. 대조적으로, 본 논문은 원하는 입력 이미지를 수용하는 개인화된 버전의 generator를 생성할 것을 제안한다. 본 논문의 접근 방식은 두 가지 주요 단계로 구성된다. 먼저 기존 inversion 기술을 사용하여 입력 이미지를 편집 가능한 latent code로 invert한다. 물론 이것은 원본과 유사하지만 반드시 동일하지는 않은 이미지를 생성한다. 두 번째 단계에서는 Pivotal Tuning을 수행한다. 이전 단계에서 찾은 pivot latent code를 사용할 때 입력 이미지가 생성되도록 사전 학습된 StyleGAN을 가볍게 튜닝한다 (위 그림 참고). 핵심 아이디어는 generator가 약간 수정되더라도 latent code가 편집 품질을 유지한다는 것이다. 수정된 generator는 전례 없는 재구성 품질을 달성하면서 pivot code의 편집 능력을 유지한다. Pivotal tuning은 latent space의 로컬한 연산을 하여 최소한의 영향으로 중추 영역의 정체성을 원하는 영역으로 이동한다. 부작용을 더욱 최소화하기 위해 정규화 항을 도입하여 latent space의 surgical adaptation만 시행한다. 이렇게 하면 간섭 없이 여러 대상 ID를 편집할 수 있는 generator가 생성된다. 

## Method
본 논문의 방법은 StyleGAN을 사용하여 실제 이미지에 대한 고품질 편집을 제공하는 것을 추구한다. 접근 방식의 핵심 아이디어는 StyleGAN의 disentangle한 특성으로 인해 강력한 편집 능력을 손상시키지 않고 생성된 모양에 약간의 로컬 변경을 적용할 수 있다는 것이다. 따라서 이미지가 외관 측면에서 분포를 벗어날 가능성이 있는 경우 generator 도메인 내에서 가장 가까운 편집 가능한 지점을 찾을 것을 제안한다. 그러면 이 중추 지점을 대상 쪽으로 끌어당길 수 있으며 주변에는 최소한의 영향만 미치고 다른 곳에서는 무시할 수 있는 영향을 미친다. 

실제 이미지를 고도로 편집 가능한 latent code로 변환하는 2단계 방법을 제시한다. 먼저 StyleGAN의 기본 latent space $\mathcal{W}$에서 주어진 입력을 $w_p$로 invert한다. 그런 다음 이 pivot code $w_p$에 Pivotal Tuning을 적용하여 사전 학습된 StyleGAN을 조정하여 입력 $w_p$에 대해 원하는 이미지를 생성한다. 여기서 직관은 $w_p$가 충분히 가깝기 때문에 pivot에서 입력 이미지를 생성하도록 generator를 학습시키는 것이 StyleGAN의 latent space의 잘 작동하는 구조에 영향을 주지 않고 모양 관련 가중치만 보강함으로써 달성될 수 있다는 것이다. 

### 1. Inversion
Inversion step의 목적은 Pivotal Tuning을 위한 편리한 시작점을 제공하는 것이다. 앞에서 언급했듯이 StyleGAN의 기본 latent space $\mathcal{W}$는 최고의 편집 가능성을 제공한다. 이로 인해 그리고 Pivotal Tuning 중에 왜곡이 줄어들기 때문에 더 널리 사용되는 $\mathcal{W}+$ 확장 대신 주어진 입력 이미지 $x$를 이 공간으로 invert시킨다. 저자들은 StyleGAN2가 제안한 기존 inversion 방법을 사용한다. 본질적으로, LPIPS perceptual loss function에 의해 측정된 입력 이미지 $x$를 재구성하기 위해 latent code $w$와 noise vector $n$ 모두를 최적화하기 위해 직접 최적화가 적용된다. StyleGAN2에서 설명한 것처럼 noise 정규화 항을 사용하여 noise vector $n$을 최적화하면 noise 정규화가 noise vector에 중요한 정보가 포함되는 것을 방지하므로 inversion이 크게 향상된다. 이는 일단 $w_p$가 결정되면 $n$ 값이 최종 시각적 모양에서 작은 역할을 한다는 것을 의미한다. 전반적으로 최적화는 다음 목적 함수로 정의된다. 

$$
\begin{equation}
w_p, n = \underset{w, n}{\arg \min} [\mathcal{L}_\textrm{LPIPS} (x, G(w, n; \theta)) + \lambda_n \mathcal{L}_n (n)]
\end{equation}
$$

여기서 $G(w, n; \theta)$는 generator $G$로 생성한 이미지이다. StyleGAN의 매핑 네트워크를 사용하지 않는다. $$\mathcal{L}_\textrm{LPIPS}$$는 perceptual loss이고 $\mathcal{L}_n$은 noise 정규화 항이다. 이 단계에서 generator는 고정된다.

### 2. Pivotal Tuning
Inversion에서 얻은 latent code $w$를 적용하면 원본 $x$와 유사한 이미지가 생성되지만 상당한 왜곡이 나타날 수 있다. 따라서 두 번째 단계에서는 generator를 고정 해제하고 pivot code $w_p$라고 하는 첫 번째 단계에서 얻은 잠재 코드 $w$가 주어지면 입력 이미지 $x$를 재구성하도록 튜닝한다. 랜덤 또는 평균 latent code를 사용하면 수렴에 실패하기 때문에 pivot code를 사용하는 것이 중요하다. $x^p = G(w_p; \theta^\ast)$를 $w_p$와 튜닝된 가중치 $\theta^\ast$를 사용하여 생성된 이미지라고 하자. 다음 loss 항을 사용하여 generator를 fine-tuning한다.

$$
\begin{equation}
\mathcal{L}_{pt} = \mathcal{L}_\textrm{LPIPS} (x, x^p) + \lambda_{L2} \mathcal{L}_{L2} (x, x^p)
\end{equation}
$$

여기서 generator는 사전 학습된 가중치 $\theta$로 초기화된다. 이 단계에서 $w_p$는 일정하다. Pivotal Tuning은 $N$개의 inverted latent code $$\{w_i\}_{i=0}^N$$이 주어지면 $N$개의 이미지 $$\{x_i\}_{i=0}^N$$으로 확장될 수 있다. 

$$
\begin{equation}
\mathcal{L}_{pt} = \frac{1}{N} \sum_{i=1}^N (\mathcal{L}_\textrm{LPIPS} (x_i, x_i^p) + \lambda_{L2} \mathcal{L}_{L2} (x_i, x_i^p)) \\
x_i^p = G (w_i; \theta^\ast)
\end{equation}
$$

Generator가 튜닝되면 latent space 편집 기술을 선택하여 입력 이미지를 편집할 수 있다.

### 3. Locality Regularization
Latent code에 Pivotal Tuning을 적용하면 실제로 generator가 입력 이미지를 높은 정확도로 재구성하고 성공적인 편집을 가능하게 한다. 동시에 Pivotal Tuning은 파급 효과를 유발한다. 즉, 로컬이 아닌 latent code에 의해 생성된 이미지의 시각적 품질이 손상된다. 이는 특히 다수의 ID에 맞게 튜닝할 때 그렇다. 이 부작용을 완화하기 위해 PTI 변경을 latent space의 로컬 영역으로 제한하도록 설계된 정규화 항을 도입한다. 각 iteration에서 정규 분포된 랜덤 벡터 $z$를 샘플링하고 StyleGAN의 매핑 네트워크 $f$를 사용하여 해당 latent code $w_z = f(z)$를 생성한다. 그런 다음 보간 파라미터 $\alpha$를 사용하여 $w_z$와 pivotal latent code $w_p$ 사이를 보간하여 보간된 code $w_r$을 얻는다. 

$$
\begin{equation}
w_r = w_p + \alpha \frac{w_z - w_p}{\| w_z - w_p \|_2}
\end{equation}
$$

마지막으로 원래 가중치를 사용하여 $w_r$을 입력으로 제공하여 생성된 이미지 $x_r = G(w_r; \theta)$와 현재 튜닝된 가중치를 사용하여 생성된 이미지 $x_r^\ast = G(w_r; \theta^\ast)$ 사이의 거리를 최소화한다.

$$
\begin{equation}
\mathcal{L}_R = \mathcal{L}_\textrm{LPIPS} (x_r, x_r^\ast) + \lambda_{L2}^R \mathcal{L}_{L2} (x_r, x_r^\ast)
\end{equation}
$$

이를 $N_r$개의 랜덤 latent code로 확장할 수 있다.

$$
\begin{equation}
\mathcal{L}_R = \frac{1}{N_r} \sum_{i=1}^{N_r} (\mathcal{L}_\textrm{LPIPS} (x_{r, i}, x_{r, i}^\ast) + \lambda_{L2}^R \mathcal{L}_{L2} (x_{r,i}, x_{r,i}^\ast))
\end{equation}
$$

새로운 최적화는 다음과 같이 정의된다.

$$
\begin{equation}
\theta^\ast  = \underset{\theta^\ast}{\arg \min} [ \mathcal{L}_{pt} + \lambda_R \mathcal{L}_R ]
\end{equation}
$$

여기서 $\lambda_{L2}^R$, $\lambda_R$, $N_r$은 hyperparameter이다. 

## Experiments
- Generator: FFHQ에서 사전 학습

### 1. Reconstruction Quality
#### Qualitative evaluation
다음은 도메인 밖의 샘플들에 대한 재구성 결과이다.

<center><img src='{{"/assets/img/pti/pti-fig4.PNG" | relative_url}}' width="60%"></center>
<br>
다음은 CelebA-HQ 데이터셋의 예시에 대하여 재구성 품질을 비교한 것이다.

<center><img src='{{"/assets/img/pti/pti-fig5.PNG" | relative_url}}' width="60%"></center>

#### Quantitative evaluation
다음은 재구성 품질에 대한 정성적 비교 결과이다. 

<center><img src='{{"/assets/img/pti/pti-table1.PNG" | relative_url}}' width="46%"></center>

### 2. Editing Quality
#### Qualitative evaluation
다음은 CelebA-HQ 데이터셋의 이미지에 대한 편집 결과를 비교한 것이다.

<center><img src='{{"/assets/img/pti/pti-fig6.PNG" | relative_url}}' width="60%"></center>
<br>
다음은 웹에서 수집한 유명인 사진을 편집한 결과를 비교한 것이다. 

<center><img src='{{"/assets/img/pti/pti-fig9.PNG" | relative_url}}' width="75%"></center>
<br>
다음은 웹에서 수집한 도메인 밖의 이미지에 대한 편집 결과를 비교한 것이다. (smile, age, beard removal)

<center><img src='{{"/assets/img/pti/pti-fig10.PNG" | relative_url}}' width="75%"></center>
<br>
다음은 순차적으로 편집을 한 예시이다. (rotation, smile)

<center><img src='{{"/assets/img/pti/pti-fig12.PNG" | relative_url}}' width="90%"></center>
<br>
다음은 추가적인 편집 예시이다. 왼쪽은 hair, pose, 중간은 hair, age, 오른쪽은 pose, smile을 편집한 것이다. 

<center><img src='{{"/assets/img/pti/pti-fig13.PNG" | relative_url}}' width="47%"></center>
<br>
다음은 Multi-ID Personalized StyleGAN을 사용한 실제 이미지 편집의 예시들이다.

<center><img src='{{"/assets/img/pti/pti-fig3.PNG" | relative_url}}' width="90%"></center>
<br>
다음은 "Friends" StyleGAN의 예시이다.

<center><img src='{{"/assets/img/pti/pti-fig11.PNG" | relative_url}}' width="90%"></center>
<br>
다음은 StyleClip을 사용하여 편집할 때 PTI 사용 여부에 대한 편집 결과를 비교한 것이다. (bowl cut, mohawk)

<center><img src='{{"/assets/img/pti/pti-fig7.PNG" | relative_url}}' width="42%"></center>
<br>
다음은 StyleClip과 InterfaceGAN을 순차적으로 사용하여 편집할 때 PTI 사용 여부에 대한 편집 결과를 비교한 것이다.  
(상단: Bob cut hair, smile, rotation / 중간: bowl cut hair, older / 하단: curly hair, younger, rotation)

<center><img src='{{"/assets/img/pti/pti-fig8.PNG" | relative_url}}' width="42%"></center>

#### Quantitative evaluation
다음은 편집 결과를 정량적으로 평가한 것이다. 

<center><img src='{{"/assets/img/pti/pti-table2.PNG" | relative_url}}' width="45%"></center>

### 3. Regularization
다음은 다중 ID에 적용되는 Pivotal Tuning을 위한 랜덤 latent code에 대한 locality regularization의 영향을 나타낸 것이다.

<center><img src='{{"/assets/img/pti/pti-fig14.PNG" | relative_url}}' width="45%"></center>
<br>
다음은 locality regularization의 정량적 평가 결과이다.

<center><img src='{{"/assets/img/pti/pti-fig15.PNG" | relative_url}}' width="50%"></center>

### 4. Ablation study
다음은 smile(상단)과 pose(하단)에 동일한 편집을 적용한 ablation study 결과이다. 

<center><img src='{{"/assets/img/pti/pti-fig16.PNG" | relative_url}}' width="100%"></center>

- (A): 본 논문의 방법과 동일
- (B): 첫 번째 단계에서 $\mathcal{W}$ 대신 $\mathcal{W}+$로 invert
- (C): Pivot latent code $w_p$를 평균 latent code $\mu_w$로 교체
- (D): $w_p$를 랜덤 latent code로 교체
- (E): $w_p$를 $\mu_w$로 초기화하고 generator와 함께 최적화
- (F): $w_p$를 랜덤 latent code로 초기화하고 generator와 함께 최적화