---
title: "[논문리뷰] One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization"
last_modified_at: 2023-12-29
categories:
  - 논문리뷰
tags:
  - Diffusion
  - Text-to-3D
  - 3D Vision
  - AI
  - NeurIPS
excerpt: "One-2-3-45 논문 리뷰 (NeurIPS 2023)"
use_math: true
classes: wide
---

> NeurIPS 2023. [[Paper](https://arxiv.org/abs/2306.16928)] [[Page](https://one-2-3-45.github.io/)] [[Github](https://github.com/One-2-3-45/One-2-3-45)]  
> Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, Hao Su  
> UC San Diego | UCLA | Cornell University | Zhejiang University | IIT Madras | Adobe  
> 29 Jun 2023  

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig1.PNG" | relative_url}}' width="95%"></center>

## Introduction
단일 이미지 3D 재구성은 단일 2D 이미지에서 물체의 3D 모델을 재구성하는 task로 컴퓨터 비전 커뮤니티에서 오랫동안 문제가 되어 왔으며 광범위한 응용 분야에 중요하다. 보이는 부분의 재구성뿐만 아니라 보이지 않는 영역의 상상도 필요하기 때문에 어려운 문제이다. 결과적으로, 이 문제는 단일 이미지의 증거가 부족하기 때문에 종종 잘못된 여러 가지 그럴듯한 결과를 만든다. 반면에 인간은 3D 세계에 대한 광범위한 지식을 바탕으로 보이지 않는 3D 콘텐츠를 능숙하게 추론할 수 있다. 이러한 능력을 부여하기 위해 기존의 많은 방법들은 3D 모양 데이터셋에서 3D 생성 네트워크를 학습시켜 클래스별 사전 분석을 활용하였다. 그러나 이러한 방법은 보이지 않는 카테고리로 일반화하는 데 실패하는 경우가 많으며 공개 3D 데이터셋의 제한된 크기로 인해 재구성 품질이 제한된다.

본 논문에서는 카테고리에 관계없이 모든 개체의 이미지를 고품질 3D 텍스처 메쉬로 변환하는 일반적인 솔루션을 추구하였다. 이를 위해 저자들은 3차원 재구성을 위해 2차원 diffusion model에서 학습된 강력한 prior를 효과적으로 활용할 수 있는 새로운 접근 방식을 제안하였다. 3D 데이터에 비해 2D 이미지는 더 쉽게 이용 가능하고 확장 가능하다. 최근 2D 생성 모델과 비전-언어 모델은 인터넷 규모의 이미지 데이터셋에 대한 사전 학습을 통해 상당한 발전을 이루었다. 이러한 모델들은 광범위한 시각적 개념을 배우고 3D 세계에 대한 강력한 prior를 갖고 있으므로 3D task를 이러한 모델들과 결합하는 것이 당연하다. 결과적으로 [DreamField](https://arxiv.org/abs/2112.01455), [DreamFusion](https://kimjy99.github.io/논문리뷰/dreamfusion), [Magic3D](https://arxiv.org/abs/2211.10440)와 같은 새로운 연구들에서는 2D diffusion model 또는 비전-언어 모델을 사용하여 3D 생성 task를 지원하였다. 이들의 일반적인 패러다임은 differentiable rendering을 통해 모양별 최적화를 수행하는 것과 CLIP 모델 또는 2D diffusion model의 guidance이다. 다른 많은 3D 표현이 탐색되었지만 neural field는 최적화 중에 가장 일반적으로 사용되는 표현이다.

이러한 최적화 기반 방법은 text-to-3D와 image-to-3D 모두에서 인상적인 결과를 얻었지만 다음과 같은 몇 가지 일반적인 딜레마에 직면한다. 

1. 많은 시간 소요: 모양별 최적화에는 일반적으로 전체 이미지 볼륨 렌더링과 prior 모델 inference를 수만 번 반복하여 모양당 일반적으로 수십 분이 소요된다. 
2. 메모리 집약적: 2D prior 모델에는 전체 이미지가 필요하므로 이미지 해상도가 높아질 때 볼륨 렌더링에 메모리 집약적일 수 있다. 
3. 3D 불일치: 2D prior 모델은 각 iteration에서 하나의 뷰만 보고 모든 뷰를 입력처럼 보이도록 시도하기 때문에 일관성이 없는 3D 모양(ex. Janus problem)을 생성하는 경우가 많다. 
4. 기하학적 구조가 좋지 않음: 많은 방법에서 density field를 볼륨 렌더링의 표현으로 활용한다. 좋은 RGB 렌더링을 생성하는 것이 일반적이지만 고품질 메쉬를 추출하는 것은 어려운 경향이 있다.

본 논문에서는 일반적인 최적화 기반 패러다임을 따르는 대신 3D 모델링을 위해 2D prior 모델을 활용하는 새로운 접근 방식을 제안하였다. 이 접근 방식의 핵심은 2D diffusion model과 cost volume 기반 3D 재구성 기술의 결합으로, 장면별 최적화 없이 feed-forward pass로 단일 이미지에서 고품질 360도 텍스처 메쉬를 재구성할 수 있도록 하는 것이다. 특히 Stable Diffusion에서 fine-tuning된 최신 2D diffusion model인 [Zero123](https://kimjy99.github.io/논문리뷰/zero-1-to-3)을 활용하여 카메라 변환에 따라 입력 이미지의 새로운 뷰를 예측한다. 이를 활용하여 하나의 입력 이미지의 멀티뷰 예측을 생성하므로 멀티뷰 3D 재구성 기술을 활용하여 3D 메쉬를 얻을 수 있다. 합성된 멀티뷰 예측으로부터의 재구성과 관련된 두 가지 과제가 있다. 

1. 멀티뷰 예측 내에서 완벽한 일관성이 본질적으로 부족하여 NeRF 방법과 같은 최적화 기반 방법에서 심각한 실패를 초래할 수 있다. 
2. 입력 이미지의 카메라 포즈가 필요하지만 알 수 없다. 

이 문제를 해결하기 위해 저자들은 cost volume 기반 neural surface 재구성 접근 방식인 [SparseNeuS](https://arxiv.org/abs/2206.05737)를 기반으로 재구성 모듈을 구축했다. 또한 본질적으로 일관되지 않은 멀티뷰 예측에서 360도 메쉬를 재구성할 수 있는 일련의 학습 전략을 도입하였다. 또한 재구성 모듈에서 요구하는 카메라 포즈를 계산하는 데 사용되는 Zero123의 표준 좌표계에서 입력 모양의 고도각을 추정하는 고도각 추정 모듈을 제안하였다. 

본 논문의 방법은 멀티뷰 합성, 고도각 추정, 3D 재구성의 세 가지 모듈을 통합함으로써 feed-forward 방식으로 단일 이미지에서 모든 객체의 3D 메쉬를 재구성할 수 있다. 또한 비용이 많이 드는 최적화 없이 훨씬 더 짧은 시간(ex. 단 45초)에 3D 모양을 재구성하며, SDF 표현을 사용하여 더 나은 geometry를 선호하고 카메라 조건부 멀티뷰 예측 덕분에 보다 일관된 3D 메쉬를 생성한다. 또한 본 논문의 재구성은 기존 방법에 비해 입력 이미지에 더 밀접하게 적용된다. 

## Method
<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig2.PNG" | relative_url}}' width="100%"></center>

### 1. Zero123: View-Conditioned 2D Diffusion
최근 2D diffusion model은 인터넷 규모 데이터에 대한 학습을 통해 광범위한 시각적 개념과 강력한 prior를 학습할 수 있는 능력을 보여주었다. 원래 diffusion model은 주로 text-to-image에 중점을 두었지만, 최근 연구에 따르면 사전 학습된 모델을 fine-tuning하면 diffusion model에 다양한 조건부 제어를 추가하고 특정 조건에 따라 이미지를 생성할 수 있는 것으로 나타났다. Canny edge, 낙서, 깊이 맵, 노멀 맵과 같은 여러 조건이 이미 효과적인 것으로 입증되었다.

[Zero123](https://kimjy99.github.io/논문리뷰/zeor-1-to-3)은 비슷한 정신을 공유하며 Stable Diffusion 모델에 시점 조건 제어를 추가하는 것을 목표로 한다. 구체적으로, Zero123는 물체의 단일 RGB 이미지와 상대적인 카메라 변환이 주어지면 diffusion model을 제어하여 이 변환된 카메라 뷰에서 새로운 이미지를 합성하는 것을 목표로 한다. 이를 위해 Zero123는 대규모 3D 데이터셋에서 합성된 상대적인 카메라 변환과 쌍을 이루는 이미지에 대해 Stable Diffusion을 fine-tuning한다. Fine-tuning 데이터셋을 생성하는 동안 Zero123는 객체가 좌표계의 원점에 중심을 두고 구형 카메라를 사용한다고 가정한다. 즉, 카메라는 구형 표면에 배치되고 항상 원점을 바라본다. 

$\theta$, $\phi$, $r$을 각각 극각(polar angle), 방위각(azimuth angle), 반경(radius)이라 할 때, 두 개의 카메라 포즈 $(\theta_1, \phi_1, r_1)$과 $(\theta_2, \phi_2, r_2)$의 경우 상대적인 카메라 변환은 $(\theta_2 − \theta_1, \phi_2 − \phi_1, r_2 − r_1)$이다. Zero123는 $f(x_1, \theta_2 − \theta_1, \phi_2 − \phi_1, r_2 − r_1)$이 $x_2$와 유사하도록 모델 $f$를 학습하는 것을 목표로 한다. 여기서 $x_1$과 $x_2$는 서로 다른 뷰에서 캡처된 객체의 두 이미지이다. Zero123는 이러한 fine-tuning을 통해 Stable Diffusion 모델이 fine-tuning 데이터셋에 표시된 개체 외부를 추정하는 카메라 시점을 제어하기 위한 일반적인 메커니즘을 학습할 수 있다. 

### 2. NeRF 최적화가 멀티뷰 예측을 3D로 향상시킬 수 있을까?
객체의 이미지가 하나 주어지면 Zero123를 활용하여 멀티뷰 이미지를 생성할 수 있지만 기존 NeRF 기반 또는 SDF 기반 방법을 사용하여 이러한 예측으로부터 고품질 3D 메쉬를 재구성할 수 있을까? 저자들은 이 가설을 검증하기 위해 작은 실험을 수행하였다. 하나의 이미지가 주어지면 먼저 Zero123를 사용하여 구 표면에서 균일하게 샘플링된 카메라 포즈를 사용하여 32개의 멀티뷰 이미지를 생성한다. 그런 다음 density field와 SDF field를 각각 최적화하는 NeRF 기반 방법([TensoRF](https://arxiv.org/abs/2203.09517))과 SDF 기반 방법([NeuS](https://arxiv.org/abs/2106.10689))에 예측을 제공한다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig3.PNG" | relative_url}}' width="100%"></center>
<br>
그러나 위 그림에서 볼 수 있듯이 두 방법 모두 만족스러운 결과를 얻지 못하여 수많은 왜곡과 floater가 생성된다. 이는 주로 Zero123의 예측이 불일치하기 때문이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig4.PNG" | relative_url}}' width="100%"></center>
<br>
위 그림은 Zero123의 예측을 ground-truth 렌더링과 비교한 것이다. 특히 상대적인 입력 포즈가 크거나 대상 포즈가 비정상적인 위치(ex. 하단, 상단)에 있는 경우 전체 PSNR이 그리 높지 않음을 알 수 있다. 그러나 mask IoU와 CLIP 유사도는 상대적으로 좋다. 이는 Zero123가 실제와 유사하고 윤곽이나 경계가 유사한 예측을 생성하는 경향이 있지만 픽셀 수준의 모양이 정확히 동일하지 않을 수 있음을 나타낸다. 그럼에도 불구하고 소스 뷰 간의 이러한 불일치는 전통적인 최적화 기반 방법에 이미 치명적이다. 원래 Zero123 논문은 멀티뷰 예측을 향상시키는 또 다른 방법을 제안했지만, 이 방법도 완벽한 결과를 얻지 못하고 시간이 많이 걸리는 최적화를 수반한다.

### 3. Neural Surface Reconstruction from Imperfect Multi-View Predictions
최적화 기반 접근 방식을 사용하는 대신, 본 논문은 멀티뷰 스테레오, neural scene 표현, 볼륨 렌더링을 결합한 파이프라인이며 일반화 가능한 SDF 재구성 방법인 SparseNeuS를 기반으로 재구성 모듈을 기반으로 한다. 재구성 모듈은 해당 카메라 포즈가 포함된 여러 소스 이미지를 입력으로 사용하고 하나의 feed-forward pass에서 텍스처 메쉬를 생성한다. 

재구성 모듈은 $m$개의 포즈 소스 이미지를 입력으로 사용한다. 모듈은 2D feature 네트워크를 사용하여 $m$개의 2D feature map을 추출하는 것으로 시작된다. 다음으로, 모듈은 먼저 각 3D voxel을 $m$개의 2D feature 평면에 project한 다음 $m$개의 project된 2D 위치에서 feature의 분산을 가져와서 콘텐츠가 계산되는 3D cost volume을 구축한다. 그런 다음 cost volume은 sparse 3D CNN을 사용하여 처리되어 입력 모양의 기본 geometry를 인코딩하는 geometry volume을 얻는다. 임의의 3D 포인트에서 SDF를 예측하기 위해 MLP 네트워크는 3D 좌표와 geometry volume에서 보간된 feature를 입력으로 사용한다. 3D 포인트의 색상을 예측하기 위해 다른 MLP 네트워크는 project된 위치의 2D feature, geometry volume에서 보간된 feature, 소스 이미지의 뷰 방향을 기준으로 한 쿼리 광선의 뷰 방향을 입력으로 사용한다. 네트워크는 각 소스 뷰의 혼합 가중치를 예측하고, 3D 포인트의 색상은 project된 색상의 가중치 합으로 예측된다. 마지막으로 RGB 및 깊이 렌더링을 위해 두 개의 MLP 네트워크 위에 SDF 기반 렌더링 기술이 적용된다.

#### 2-Stage Source View Selection and Groundtruth-Prediction Mixed Training
[SparseNeuS](https://arxiv.org/abs/2206.05737) 논문은 정면 뷰 재구성만 시연했지만 저자들은 특정 방식으로 소스 뷰를 선택하고 학습 중에 깊이 supervision을 추가하여 하나의 feed-forward pass에서 360도 메쉬를 재구성하도록 확장했다. 특히, 재구성 모델은 Zero123를 고정하고 3D 객체 데이터셋에 대해 학습된다. Zero123를 따라 모양을 정규화하고 구형 카메라 모델을 사용한다. 각 모양에 대해 먼저 구에 균일하게 배치된 $n$개의 카메라 포즈의 $n$개의 ground-truth RGB 및 깊이 이미지를 렌더링한다. $n$개의 뷰 각각에 대해 Zero123를 사용하여 4개의 인근 뷰를 예측한다. 학습 중에 ground-truth 포즈가 포함된 $4 \times n$개의 예측을 모두 재구성 모듈에 공급하고 $n$개의 ground-truth RGB 이미지 뷰 중 하나를 타겟 뷰로 무작위로 선택한다. 이 뷰 선택 전략을 2-stage source view selection이라고 부르며 ground-truth RGB와 깊이 값을 모두 사용하여 학습한다. 이러한 방식으로 모듈은 Zero123의 일관되지 않은 예측을 처리하고 일관된 360도 메쉬를 재구성하는 방법을 학습할 수 있다. 구 표면에서 $n \times 4$개의 소스 뷰를 균일하게 선택하면 카메라 포즈 사이의 거리가 더 길어지기 때문에 2-stage source view selection 전략이 중요하다. 그러나 cost volume 기반 방법은 일반적으로 로컬한 이미지들을 찾기 위해 매우 가까운 소스 뷰에 의존한다. 또한 상대적인 포즈가 작은 경우 (ex. 10도 간격) Zero123는 매우 정확하고 일관된 예측을 제공할 수 있으므로 로컬한 이미지들을 찾고 geometry를 추론하는 데 사용할 수 있다.

학습 중에 더 나은 supervision을 위해 첫 번째 단계에서 $n$개의 ground-truth 렌더링을 사용하여 dpeth loss를 활성화한다. 그러나 inference 중에 $n$개의 ground-truth 렌더링을 Zero123 예측으로 대체할 수 있으며 깊이 입력이 필요하지 않다. 텍스처가 있는 메쉬를 내보내려면 marching cube를 사용하여 예측된 SDF field에서 메쉬를 추출하고 Neus에 설명된 대로 메쉬 정점의 색상을 쿼리한다. 재구성 모듈은 3D 데이터셋으로 학습되었지만 주로 로컬한 이미지들에 의존하고 보이지 않는 모양을 매우 잘 일반화할 수 있다. 

### 4. Camera Pose Estimation
재구성 모듈에는 $4 \times n$개의 소스 뷰 이미지에 대한 카메라 포즈가 필요하다. Zero123는 표준 구면 좌표계 $(\theta, \phi, r)$에서 카메라를 parameterize한다. 모든 소스 뷰 이미지의 $\phi$와 $r$을 동시에 임의로 조정하여 재구성된 객체의 회전 및 크기 조정을 수행할 수 있지만, 모든 카메라의 상대적 포즈를 결정하기 위해 한 카메라의 절대적인 고도각 $\theta$를 알아야 한다. 보다 구체적으로, 카메라 $(\theta_0, \phi_0, r_0)$와 카메라 $(\theta_0 + \Delta \theta, \phi_0 + \Delta \phi, r_0)$ 사이의 상대적인 포즈는 $\Delta \theta$와 $\Delta \phi$가 동일하더라도 $\theta_0$에 따라 다르다. 이로 인해 모든 소스 이미지의 $\theta_0$를 함께 변경하면 재구성된 모양이 왜곡된다. 

따라서 저자들은 입력 이미지의 고도각을 추론하기 위한 고도각 추정 모듈을 제안하였다. 먼저 Zero123를 사용하여 입력 이미지의 인근 뷰 4개를 예측한다. 그런 다음 가능한 모든 앙각을 coarse-to-fine 방식으로 열거한다. 각 고도각 후보에 대해 4개의 이미지에 해당하는 카메라 포즈를 계산하고 이 카메라 포즈 집합에 대한 reprojection 오차를 계산하여 이미지와 카메라 포즈 간의 일관성을 측정한다. Reprojection 오차가 가장 작은 고도각은 입력 뷰의 포즈와 상대 포즈를 결합하여 $4 \times n$개의 소스 뷰 전체에 대한 카메라 포즈를 생성하는 데 사용된다. 

## Experiments
- 데이터셋: Objaverse-LVIS
- 구현 디테일
  - 각 입력 이미지에 대해 구 표면에 균일하게 배치된 카메라 포즈를 선택하여 $n = 8$개의 이미지를 생성한 다음 8개의 뷰 각각에 대해 4개의 로컬 이미지(10도 간격)를 생성하여 32개의 소스 뷰 이미지를 생성
  - 학습 중에 Zero123 모델을 고정하고 Objaverse-LVIS에서 재구성 모듈을 학습
  - 실제 RGB 및 깊이 이미지를 렌더링하기 위해 BlenderProc를 사용
  - 배경이 있는 이미지의 경우 배경 제거를 위한 [SAM](https://kimjy99.github.io/논문리뷰/segment-anything)을 활용

### 1. Single Image to 3D Mesh
다음은 합성 이미지와 실제 이미지에 대한 One-2-3-45의 예시이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig5.PNG" | relative_url}}' width="100%"></center>
<br>
다음은 One-2-3-45과 다른 방법들의 결과를 비교한 것이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig6.PNG" | relative_url}}' width="100%"></center>
<br>
다음은 GSO와 Objaverse 데이터셋에서 정량적으로 비교한 표이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-table1.PNG" | relative_url}}' width="60%"></center>

### 2. Ablation Study
다음은 재구성 모듈의 학습 전략과 뷰의 수에 따른 ablation 결과이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig8.PNG" | relative_url}}' width="100%"></center>
<br>
다음은 잘못된 고도각에 의한 영향을 비교한 것이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig10.PNG" | relative_url}}' width="55%"></center>
<br>
다음은 예측된 고도각의 오차 분포이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig7.PNG" | relative_url}}' width="30%"></center>
<br>
다음은 360도 재구성과 멀티뷰 융합을 비교한 것이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig9.PNG" | relative_url}}' width="50%"></center>

### 3. Text to 3D Mesh
다음은 Text to 3D 결과이다. 첫번째 행의 입력은 "a bear in cowboy suit"이고 두번째 행의 입력은 "a kungfu cat"이다. 

<center><img src='{{"/assets/img/one-2-3-45/one-2-3-45-fig11.PNG" | relative_url}}' width="100%"></center>