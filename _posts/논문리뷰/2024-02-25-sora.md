---
title: "[Technical Report 리뷰] Video generation models as world simulators (Sora)"
last_modified_at: 2024-02-25
categories:
  - 논문리뷰
tags:
  - Computer Vision
  - Text-to-Video
  - Video Generation
  - Image Generation
  - AI
  - OpenAI
excerpt: "Sora Technical Report 리뷰"
use_math: true
classes: wide
---

> arXiv 2024. [[Page](https://openai.com/research/video-generation-models-as-world-simulators)]  
> Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, Aditya Ramesh  
> OpenAI  
> 16 Feb 2024  

<center><video autoplay controls loop muted playsinline="true" src="https://cdn.openai.com/tmp/s/title_0.mp4" width="90%"></video></center>

## Introduction
이 technical report는 모든 유형의 시각적 데이터를 생성 모델의 대규모 학습을 가능하게 하는 통합 표현으로 변환하는 방법과 Sora의 능력과 한계에 대한 질적 평가에 중점을 두었다. 모델 및 구현 디테일은 다루지 않았다. 

이전 연구들에서는 RNN, GAN, transformer, diffusion model을 포함한 다양한 방법을 사용하여 동영상 데이터의 생성적 모델링을 연구했다. 이러한 연구들은 시각적 데이터의 좁은 카테고리, 짧은 동영상 또는 고정된 크기의 동영상에 초점을 맞추는 경우가 많다. Sora는 시각적 데이터의 generalist model이며, 다양한 길이, 종횡비, 해상도의 이미지 및 동영상 부터 최대 1분 분량의 고화질 동영상까지 생성할 수 있다. 

## Method
### Turning visual data into patches
저자들은 인터넷 규모 데이터에 대한 학습을 통해 일반화 능력을 얻는 대규모 언어 모델(LLM)에서 영감을 얻다. LLM 패러다임의 성공은 부분적으로 텍스트의 다양한 모달리티(코드, 수학, 다양한 언어)를 우아하게 통합하는 토큰의 사용에 의해 가능해졌다. 저자들은 시각적 데이터의 생성 모델이 이러한 이점을 어떻게 상속받을 수 있는지 고려하였다. LLM에는 텍스트 토큰이 있는 반면 Sora에는 비주얼 패치가 있다. 이전 연구들에서 패치는 시각적 데이터를 다루는 모델을 위한 효과적인 표현인 것으로 나타났다. 저자들은 패치가 다양한 유형의 동영상 및 이미지에 대한 생성 모델을 학습시키기 위한 확장성이 뛰어나고 효과적인 표현이라는 것을 발견했다.

<center><img src='{{"/assets/img/sora/sora-fig1.PNG" | relative_url}}' width="75%"></center>
<br>
높은 레벨에서 먼저 동영상을 낮은 차원의 latent space로 압축한 다음 표현을 시공간 패치로 분해하여 동영상을 패치로 변환한다.

### Video compression network
저자들은 시각적 데이터의 차원을 줄이는 네트워크를 학습시켰다. 이 네트워크는 동영상을 입력으로 사용하고 시공간적으로 압축된 latent 표현을 출력한다. Sora는 이 압축된 latent space 내에서 학습을 받은 후 동영상을 생성한다. 또한 생성된 latent를 다시 픽셀 공간에 매핑하는 디코더 모델을 학습시킨다. 

### Spacetime latent patches
압축된 동영상 입력이 주어지면 transformer 토큰 역할을 하는 일련의 시공간 패치를 추출한다. 이미지는 프레임이 하나인 동영상일 뿐이므로 이 방식은 이미지에도 적용된다. Sora는 패치 기반 표현을 통해 다양한 해상도, 길이, 종횡비의 동영상 및 이미지를 학습할 수 있다. inference 시, 적절한 크기의 그리드에 무작위로 초기화된 패치를 배열하여 생성된 동영상의 크기를 제어할 수 있다. 

### Scaling transformers for video generation
Sora는 noisy한 입력 패치(및 텍스트 프롬프트와 같은 컨디셔닝 정보)가 주어지는 diffusion model이며 원래의 깨끗한 패치를 예측하도록 학습되었다. 중요한 것은 Sora가 [diffusion transformer](https://kimjy99.github.io/논문리뷰/dit/)라는 것이다. Transformer는 언어 모델링, 컴퓨터 비전, 이미지 생성을 포함한 다양한 도메인에 걸쳐 놀라운 스케일링 속성을 보여주었다.

<center><img src='{{"/assets/img/sora/sora-fig2.PNG" | relative_url}}' width="75%"></center>
<br>
저자들은 diffusion transformer가 동영상 모델로도 효과적으로 스케일링된다는 것을 발견했다. 아래 세 동영상은 학습이 진행됨에 따라 고정된 시드와 입력으로 생성한 동영상 샘플을 비교한 것이다. 학습 계산량이 증가함에 따라 샘플 품질이 눈에 띄게 향상된다. 

<div style="display: flex;">
    <div>
        <video autoplay controls loop muted playsinline="true" src="https://cdn.openai.com/tmp/s/scaling_0.mp4" width="100%"></video>
        <br>Base compute
    </div>
    <div>
        <video autoplay controls loop muted playsinline="true" src="https://cdn.openai.com/tmp/s/scaling_1.mp4" width="100%"></video>
        <br>4x compute
    </div>
    <div>
        <video autoplay controls loop muted playsinline="true" src="https://cdn.openai.com/tmp/s/scaling_2.mp4" width="100%"></video>
        <br>32x compute
    </div>
</div>

### Variable durations, resolutions, aspect ratios
이미지 및 동영상 생성에 대한 과거 접근 방식은 일반적으로 동영상의 크기를 조정하거나 표준 크기로 자르거나 다듬는다. 이는 데이터를 원래 크기로 학습하는 것에 비해 여러 가지 이점을 제공한다. 

1. **샘플링 유연성**: Sora는 와이드스크린 1920$\times$1080 동영상, 1080$\times$1920 세로 동영상, 그리고 그 사이의 모든 동영상을 샘플링할 수 있다. 이를 통해 Sora는 다양한 기기의 기본 종횡비에 맞춰 콘텐츠를 직접 만들 수 있다. 또한 동일한 모델을 사용하여 전체 해상도로 생성하기 전에 더 작은 크기의 콘텐츠 프로토타입을 빠르게 제작할 수 있다.
2. **향상된 프레이밍 및 합성**: 저자들은 기본 종횡비로 동영상을 학습시키면 합성과 프레이밍이 향상된다는 것을 경험적으로 발견했다. 

## Results
### Language understanding
Text-to-videp 생성 시스템을 학습시키려면 해당 텍스트 캡션이 포함된 대량의 동영상이 필요하다. [DALL·E 3](https://cdn.openai.com/papers/dall-e-3.pdf)에서 도입된 re-captioning 기술을 영상에 적용한다. 먼저 고도로 서술적인 캡션 작성 모델을 학습시킨 다음 이를 사용하여 학습 세트의 모든 동영상에 대한 텍스트 캡션을 생성한다. 설명이 풍부한 동영상 캡션에 대한 학습을 통해 동영상의 전반적인 품질은 물론 텍스트 충실도도 향상된다. 

DALL·E 3와 유사하게 GPT를 활용하여 짧은 사용자 프롬프트를 동영상을 위한 더 길고 자세한 캡션으로 변환한다. 이를 통해 Sora는 사용자의 지시를 정확하게 따르는 고품질 동영상을 생성할 수 있다. 

### Prompting with images and videos
Sora는 기존 이미지나 동영상과 같은 다른 입력을 통해 프롬프팅할 수도 있다. 이 기능을 통해 Sora는 완벽하게 반복되는 동영상 생성, 정적 이미지 애니메이션, 동영상 시간을 앞뒤로 확장하는 등 광범위한 이미지 및 동영상 편집 작업을 수행할 수 있다. Sora로 수행할 수 있는 작업은 다음과 같다. 

1. **DALL·E 이미지 애니메이션**: 이미지와 프롬프트를 입력으로 받아 동영상을 생성할 수 있다. 
2. **생성된 동영상 확장**: 영상을 앞이나 뒤로 확장할 수 있다. 
3. **Video-to-video 편집**: [SDEdit](https://arxiv.org/abs/2108.01073)을 Sora에 적용하여 입력 동영상의 스타일과 환경을 zero-shot으로 변환할 수 있다.
4. **동영상 연결**: 두 개의 입력 동영상 사이를 점진적으로 보간하여 완전히 다른 주제와 장면 구성이 있는 동영상 사이에 원활한 전환을 생성할 수 있다. 

### Image generation capabilities
Sora는 한 프레임의 시간적 범위를 갖는 공간 그리드에 Gaussian noise의 패치들을 배열하여 이미지를 생성할 수 있다. 최대 2048$\times$2048 해상도까지 다양한 크기의 이미지를 생성할 수 있다.

<center><img src='{{"/assets/img/sora/sora-fig3.PNG" | relative_url}}' width="85%"></center>

### Emerging simulation capabilities
저자들은 동영상 모델이 대규모로 학습될 때 여러 가지 흥미로운 능력을 보여준다는 것을 발견했다. 이러한 능력을 통해 Sora는 물리적 세계에서 사람, 동물 및 환경의 일부 측면을 시뮬레이션할 수 있다. 이러한 속성은 3D, 물체 등에 대한 명시적인 inductive bias 없이 나타나며, 이는 순전히 스케일에 따른 현상이다. 

1. **3D 일관성**: Sora는 역동적인 카메라 움직임으로 동영상을 생성할 수 있다. 카메라가 이동하고 회전하면 사람과 장면 요소가 3차원 공간에서 일관되게 움직인다.
2. **장거리 일관성 및 물체 불변성**: 동영상 생성 시스템의 중요한 과제는 긴 동영상을 샘플링할 때 시간적 일관성을 유지하는 것이었다. Sora는 항상 그런 것은 아니지만 종종 단기 및 장거리 의존성을 효과적으로 모델링할 수 있다. 예를 들어, 사람, 동물, 물체가 가려지거나 프레임을 벗어나는 경우에도 이를 유지할 수 있다. 마찬가지로, 하나의 샘플에서 동일한 캐릭터의 여러 장면을 생성하여 동영상 전체에서 해당 모습을 유지할 수 있다.
3. **물리적 세계의 상호작용**: Sora는 때때로 간단한 방법으로 물리적 세계의 상태에 영향을 미치는 행동을 시뮬레이션할 수 있다. 예를 들어, 화가는 캔버스에 시간이 지나도 지속되는 새로운 선을 남길 수 있고, 남자는 햄버거를 먹고 물린 자국을 남길 수 있다. 
4. **디지털 세계 시뮬레이션**: Sora는 인공적인 프로세스를 시뮬레이션할 수도 있다. 한 가지 예는 컴퓨터 게임이다. Sora는 Minecraft의 플레이어를 제어하는 동시에 디지털 세계와 그 역학을 충실하게 렌더링할 수 있다. 이러한 능력은 "Minecraft"를 언급하는 캡션을 Sora에게 프롬프팅하여 zero-shot으로 유도할 수 있다.

## Limitation
1. 유리가 깨지는 것과 같은 많은 기본 상호 작용의 물리학을 정확하게 모델링하지 않는다. 
2. 음식 먹기와 같은 다른 상호 작용이 항상 물체 상태에 올바른 변화를 가져오는 것은 아니다. 
3. 장기간의 샘플에서 불일치나 객체의 자발적인 출현 등이 발생한다. 