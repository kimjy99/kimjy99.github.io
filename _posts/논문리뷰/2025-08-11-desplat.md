---
title: "[논문리뷰] DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering"
last_modified_at: 2025-08-11
categories:
  - 논문리뷰
tags:
  - Gaussian Splatting
  - Novel View Synthesis
  - 3D Vision
  - CVPR
excerpt: "DeSplat 논문 리뷰 (CVPR 2025)"
use_math: true
classes: wide
---

> CVPR 2025. [[Paper](https://arxiv.org/abs/2411.19756)] [[Page](https://aaltoml.github.io/desplat/)] [[Github](https://github.com/AaltoML/desplat/)]  
> Yihao Wang, Marcus Klasson, Matias Turkulainen, Shuzhe Wang, Juho Kannala, Arno Solin  
> Technical University of Munich | Aalto University | University of Oulu  
> 29 Nov 2024  

<center><img src='{{"/assets/img/desplat/desplat-fig2.webp" | relative_url}}' width="100%"></center>

## Introduction
기존의 [3DGS](https://kimjy99.github.io/논문리뷰/3d-gaussian-splatting) 및 [NeRF](https://kimjy99.github.io/논문리뷰/nerf)를 이용한 방해 요소(distractor)를 제거한 렌더링 방법들은 이미지의 어떤 픽셀이 distractor에 속하는지 감지하고 최적화 중에 그 영향을 줄이는 것을 목표로 한다. 본 논문에서는 3D 장면 표현을 각각 정적 장면과 distractor를 재구성하는 두 개의 서로 다른 Gaussian 집합으로 분해하는 방법인 **DeSplat**을 제안하였다. 정적 Gaussian과 distractor Gaussian의 명시적 모델링을 통해 볼륨 렌더링을 두 부분으로 분해하고, 3DGS와 동일한 photometric loss를 활용하여 시점별 distractor 학습과 정적 3D 장면 최적화로 분리한다. 이러한 분리는 추가적인 전처리 단계 없이 빠른 렌더링 성능을 유지한다.

## Method
<center><img src='{{"/assets/img/desplat/desplat-fig3.webp" | relative_url}}' width="100%"></center>
<br>
정적 장면과 distractor를 명시적으로 모델링하기 위해 3DGS 표현을 분해하여 각각을 재구성하도록 최적화된 두 개의 Gaussian 점 $$\mathcal{G}_s$$와 $$\mathcal{G}_d$$ 세트를 초기화한다. 주어진 시점의 출력 픽셀 색상을 정적 장면과 distractor에 해당하는 두 개의 이미지로 분해하여 이미지를 렌더링한다. 

$$
\begin{equation}
\textbf{c}_\textrm{comp} = \textbf{c}_d + (1 - \alpha_d) \textbf{c}_s \\
\alpha_d = \prod_{i=1}^{N_d} \alpha_{d,i} \prod_{j=1}^{i-1} (1 - \alpha_{d,j})
\end{equation}
$$

이러한 분리를 통해 정적 장면 요소와 distractor를 독립적으로 렌더링할 수 있다.

##### Per-view Distractor Gaussian
Distractor가 여러 시점에 나타나거나 프레임 간에 약간씩 이동하는 경우, 장면을 정적 Gaussian과 distractor Gaussian으로 분해하는 것은 어렵다. Distractor가 종종 시점에 따라 달라지는 효과로 동작하기 때문에 글로벌하게 공유되는 distractor Gaussian 집합은 효과적이지 않다. 따라서 각 $n$번째 시점에 대한 distractor Gaussian 집합을 $$\mathcal{G}_{d,n}$$으로 초기화한다. 시점별 distractor Gaussian은 최적화 과정에서 해당 카메라 시점이 선택된 경우에만 최적화된다.

##### Distractor Gaussian의 초기화
카메라 뷰 앞의 2D 평면에 distractor Gaussian을 배치하여 뷰당 distractor Gaussian의 위치를 ​​초기화한다. Camera-to-world rotation matrix가 $\textbf{R}$이고 translation vector $\textbf{t}$인 단일 뷰에 대해, distractor Gaussian의 위치는 

$$
\begin{equation}
\boldsymbol{\mu}_d = \textbf{t} - \textbf{R}^\top [\rho u, \rho v, \rho]^\top \quad u, v \sim \mathcal{U}(0,1)
\end{equation}
$$

상수 $\rho$는 깊이를 제어하며, 실험에서는 $\rho = 0.02$를 사용한다. 다른 Gaussian 파라미터는 3DGS의 표준 초기화 방식을 따른다. 각 학습 카메라 뷰에 대해 고정된 수의 distractor Gaussian이 초기화된다.

##### Per-view Densification
학습 이미지에서 distractor의 개수와 면적이 변하는 경우, 뷰별 distractor Gaussian에 Adaptive Density Control (ADC)를 적용한다. 3DGS의 ADC가 $T$ iteration마다 Gaussian들을 densification하는 것과 달리, 해당 이미지가 $S$번 학습될 때마다 distractor Gaussian을 densification한다. Distractor Gaussian과 정적 Gaussian 모두 3DGS와 동일한 컬링, 분할, 복제 절차를 사용한다.

##### Method Pipeline
정적 Gaussian들은 3DGS에서 일반적으로 수행되는 것처럼 COLMAP에서 얻은 sparse 포인트 클라우드를 사용하여 글로벌하게 초기화되는 반면, distractor Gaussian 포인트는 각 카메라 뷰 앞에서 로컬로 초기화된다. 정적 Gaussian과 distractor Gaussian은 별도로 rasterization되어 각각 정적 요소와 distractor를 나타내는 두 이미지로 렌더링된다. 두 이미지의 알파 블렌딩을 통해 합성 이미지를 얻고, 이를 GT 이미지와 비교하여 photometric loss를 계산한다. 정적 요소와 distractor를 모두 재구성함으로써 둘을 독립적으로 렌더링할 수 있는 명시적 장면 분리를 얻는다.

##### Loss Function 
Distractor 분리를 향상시키기 위해 distractor Gaussian의 불투명도와 alpha map에 정규화를 적용한다. 이는 가능한 한 적은 수의 Gaussian을 사용하여 distractor를 재구성하기 위함이며, 최종 loss는 다음과 같다.

$$
\begin{equation}
\mathcal{L} = (1 - \lambda) \mathcal{L}_1 + \lambda \mathcal{L}_\textrm{D-SSIM} + \lambda_s \vert 1 - \alpha_s \vert + \lambda_d \vert \alpha_d \vert
\end{equation}
$$

정규화 항들은 distractor Gaussian을 사용하여 정적 물체를 재구성하는 것을 방해하고, 정적 Gaussian의 누적 값이 1이 되도록 유도하여 배경에 빈 공간이 발생할 위험을 줄인다.

## Experiments
- 데이터셋: RobustNeRF, NeRF On-the-go, Photo Tourism
- 구현 디테일
  - 정적 Gaussian은 3DGS와 동일
  - 각 학습 이미지마다 $K = 1000$개의 distractor Gaussian으로 초기화
  - densification 주기: $S = 10$
  - optimizer: Adam
  - learning rate: quaternion과 scale은 3DGS의 10배, 나머지는 그대로 유지
  - 색상은 SH를 사용하지 않고 단순히 3차원 RGB로 모델링
  - opacity reset은 하지 않음
  - RGB 색상에 sigmoid function을 사용하지 않고 단순히 clamping
  - loss 가중치: $$\lambda_d = \lambda_s = 0.01$$

### 1. Distractor-free 3D Reconstruction
다음은 RobustNeRF 데이터셋에서의 비교 결과이다.

<center><img src='{{"/assets/img/desplat/desplat-fig5.webp" | relative_url}}' width="100%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/desplat/desplat-table2.webp" | relative_url}}' width="100%"></center>
<br>
다음은 NeRF On-the-go 데이터셋에서의 비교 결과이다.

<center><img src='{{"/assets/img/desplat/desplat-fig6.webp" | relative_url}}' width="100%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/desplat/desplat-table3.webp" | relative_url}}' width="100%"></center>
<br>
다음은 Photo Tourism 데이터셋에서의 결과이다. 

<center><img src='{{"/assets/img/desplat/desplat-fig7.webp" | relative_url}}' width="70%"></center>

### 2. Ablation Studies
다음은 distractor Gaussian, ADC, 정규화에 대한 ablation 결과이다. 

<center><img src='{{"/assets/img/desplat/desplat-fig8.webp" | relative_url}}' width="60%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/desplat/desplat-table4.webp" | relative_url}}' width="65%"></center>
<br>
다음은 distractor 비율과 초기 Gaussian 개수에 대한 ablation 결과이다. 

<center><img src='{{"/assets/img/desplat/desplat-fig9.webp" | relative_url}}' width="55%"></center>
