---
title: "[논문리뷰] IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images"
last_modified_at: 2025-10-16
categories:
  - 논문리뷰
tags:
  - NeRF
  - Novel View Synthesis
  - 3D Vision
  - CVPR
  - Meta
excerpt: "IRIS 논문 리뷰 (CVPR 2025)"
use_math: true
classes: wide
---

> CVPR 2025. [[Paper](https://arxiv.org/abs/2401.12977)] [[Page](https://irisldr.github.io/)] [[Github](https://github.com/facebookresearch/iris)]   
> Chih-Hao Lin, Jia-Bin Huang, Zhengqin Li, Zhao Dong, Christian Richardt, Tuotuo Li, Michael Zollhöfer, Johannes Kopf, Shenlong Wang, Changil Kim  
> Meta | University of Illinois Urbana-Champaign | University of Maryland College Park  
> 23 Jan 2025  

<center><img src='{{"/assets/img/iris/iris-fig1.webp" | relative_url}}' width="100%"></center>

## Introduction
물리 기반 inverse rendering을 통해 장면에서 사실적인 material 속성과 조명(lighting)을 재구성할 수 있다. 이러한 분해는 relighting, material 편집, 사실적인 물체 삽입 등 다양한 응용 분야에 유용하다. 그러나 대부분의 기존 inverse rendering 방식은 장면의 전체 light transport를 포착하기 위해 HDR 입력 이미지를 필요로 한다.

대부분의 이미징 센서는 장면의 발광 영역과 어두운 영역에 대한 충분한 dynamic range를 포착하지 못한다. 또한, 카메라는 센서 판독값을 8비트 LDR 이미지로 변환하는 경우가 많다. 비선형 매핑과 quantization은 조명 정보의 추가적인 손실을 초래한다. 실내 환경에서의 복잡한 light transport는 충실한 inverse rendering에 필수적인 원래 HDR 조명을 재구성하는 것을 더욱 어렵게 만든다.

SOTA inverse rendering 방법 중 다수는 HDR 입력을 사용하여 material 속성과 조명을 정확하게 복원한다. 일부 방법은 LDR 이미지를 입력으로 사용하여 조명을 계산함으로써 이 문제를 해결하려고 시도하였다. 그러나 이러한 기법은 무한히 먼 거리의 광원, emitter mask와 같은 추가 입력, multi-bounce light transport와 같은 단순화된 가정을 하는 경우가 많다. 결과적으로 이러한 방법들은 실내 장면에서 복잡한 light transport를 처리하는 데 어려움을 겪고, 고품질 표면 material과 조명을 생성하지 못하는 경우가 많다.

본 논문은 이러한 과제를 해결하기 위해, 멀티뷰 LDR 이미지만을 사용하는 실내 장면 inverse rendering 렌더링 방법인 **IRIS**를 소개한다. IRIS는 tone mapping (HDR-LDR 변환)을 모델링하여 LDR 입력을 직접 처리할 수 있도록 한다. IRIS는 물리 기반 inverse rendering을 통해 자동으로 emitter를 식별하고 HDR 조명을 재구성한다. 이는 정확한 material 추정에 필수적이다. 조명, 표면 material, camera response function(CRF)을 동시에 추정하는 것은 모호성으로 인해 최적화가 불안정해지는 결과를 초래한다. 이 문제를 해결하기 위해, 본 연구에서는 이러한 모호성을 효과적으로 해결하고 고품질 추정을 가능하게 하는 새로운 최적화 전략을 설계했다.

## Representation and rendering
#### Camera response function (CRF)
CRF는 8비트 LDR 이미지에서 원래 radiance를 복원하는 데 필수적이다. 그러나 CRF는 카메라마다 다르며 사용자에게 알려지지 않은 경우가 많다. 따라서 LDR 이미지에서 CRF를 추론하는 것이 본 논문의 핵심이다. 본 논문에서는 CRF 모델링에 EMoR을 사용한다. EMoR은 다양한 기기의 201개 실제 CRF 데이터베이스를 수집하여 벡터로 parameterize했다. 데이터베이스에서 평균 곡선 $$\bar{\textbf{g}}$$와 PCA basis $$\textbf{g}_b$$를 계산하고, 학습 가능한 CRF를 다음과 같이 parameterize한다.

$$
\begin{equation}
\textbf{g} = \bar{\textbf{g}} + \sum_b w_b \textbf{g}_b, \quad \textbf{g} \in \mathbb{R}^{1024}
\end{equation}
$$

$$\{w_b\}$$는 각 basis 곡선의 가중치를 부여하는 학습 가능한 파라미터이다. 비선형인 CRF는 

$$
\begin{equation}
\textrm{CRF}(x) = \textrm{LERP}(\{\textbf{g}_k\}_k, x)
\end{equation}
$$

로 discretize되며, 이는 균일하게 샘플링된 CRF $$\{\textbf{g}_k\}_{k=0}^{1023} \in [0, 1]^{1024}$$를 색상 채널별로 linear interpolation한다. 이러한 설계를 선택한 이유는 CRF 공간의 대부분을 포괄하고, 단순성 덕분에 정규화를 쉽게 적용할 수 있기 때문이며, 이는 inverse rendering의 복잡한 최적화에 큰 이점을 제공한다.

#### Spatially-varying lighting
Environment map은 물체 중심 및 실외 장면을 잘 처리하지만 실내 장면의 복잡하고 공간적으로 변하는 조명을 포착할 수 없다. 이를 처리하기 위해 장면 메쉬에 직접 조명을 정의하고 emission을 view-independent
radiance $$\textbf{L}_e (\textbf{x}) \in \mathbb{R}^3$$로 측정한다. 또한 표면 위의 점 $\textbf{x}$가 emitter에 있는지 여부를 나타내기 위해 emitter mask $$M_e (\textbf{x}) \in \{0, 1\}$$을 정의한다. Global illumination은 view-independent한 surface light field (SLF) $$\textbf{L}_\textrm{SLF}(\textbf{x}) \in \mathbb{R}^3$$으로 표현된다. 

입력 LDR 이미지의 sRGB 픽셀 색상은 inverse CRF 매핑으로 선형화된다. 

$$
\begin{equation}
\textbf{I}_\textrm{linear} = \textrm{CRF}^{-1} (\textbf{I}_\textrm{LDR})
\end{equation}
$$

Voxel에 표면이 없으면 0으로 설정되고, 그렇지 않으면 ray casting으로 모든 입력 이미지에서 선형화된 평균 radiance를 수집한다. 이렇게 하면 multi-bounce ray tracing이 필요 없이 global illumination의 근사가 크게 가속화된다.

#### Material BRDF
본 논문은 material을 Cook–Torrance BRDF를 사용하여 표현하였다. 표면 albedo $\textbf{a}(\textbf{x}) \in \mathbb{R}^3$, roughness $\sigma(\textbf{x}) \in \mathbb{R}$, metallicity $m(\textbf{x}) \in \mathbb{R}$은 diffuse reflectance $$\textbf{k}_d$$와 specular reflectance $$\textbf{k}_s$$를 모델링하며, multi-resolution 해시 인코딩과 MLP로 구현된 neural field $\textbf{f} : \textbf{x} \mapsto (\textbf{a}, \sigma, m)$으로 parameterize된다.

$$
\begin{aligned}
\textbf{k}_d &= \textbf{a} \cdot (1 - m) \\
\textbf{k}_s &= 0.04 \cdot (1-m) + \textbf{a} \cdot m
\end{aligned}
$$

#### Factorized light transport
장면의 geometry, material, 조명 정보로부터 이미지를 현실적으로 렌더링하기 위해, 다음과 같은 렌더링 방정식을 따른다.

$$
\begin{aligned}
\textbf{L}_o (\textbf{x}, \boldsymbol{\omega}_o) &= \textbf{L}_e (\textbf{x}, \boldsymbol{\omega}_o) + \textbf{L}_r (\textbf{x}, \boldsymbol{\omega}_o) \\
\textbf{L}_r (\textbf{x}, \boldsymbol{\omega}_o) &= \int_{\Omega_{+}} \textbf{L}_i (\textbf{x}, \boldsymbol{\omega}_i) f(\textbf{x}, \boldsymbol{\omega}_i, \boldsymbol{\omega}_o) d \boldsymbol{\omega}_i
\end{aligned}
$$

($$\textbf{L}_o$$는 3D 위치 $\textbf{x}$에서 $$\boldsymbol{\omega}_o$$ 방향으로 광선을 따라 관찰되는 radiance, $f$는 BRDF)

렌더링 방정식의 재귀적 특성은 Monte–Carlo path tracing으로 근사된다. 그러나 여러 번의 반사를 모델링하는 것은 여전히 ​​계산적으로 비용이 많이 들고 inverse rendering 프로세스에서 불안정하다. 최적화의 효율성과 robustness를 높이기 위해 [FIPT](https://arxiv.org/abs/2304.05669)를 따라 factorized light transport를 채택한다. 즉, shading map $$\textbf{L}_d$$, $$\textbf{L}_s^0$$, $$\textbf{L}_s^1$$을 미리 계산하고 roughness를 사용하여 linear interpolation하여 최종 렌더링을 얻는다.

$$
\begin{aligned}
\textbf{L}_r (\textbf{x}, \boldsymbol{\omega}_o) &= \textbf{k}_d \textbf{L}_d (\textbf{x}) + \textbf{k}_s \textbf{L}_s^0 (\textbf{x}, \boldsymbol{\omega}_o, \sigma) + \textbf{L}_s^1 (\textbf{x}, \boldsymbol{\omega}_o, \sigma) \\
\textbf{L}_s^0 (\textbf{x}, \boldsymbol{\omega}_o, \sigma) &= \textrm{LERF} (\{\textbf{L}_s^0 (\textbf{x}, \boldsymbol{\omega}_o, \sigma_i) \}_{i=1}^6, \sigma) \\
\textbf{L}_s^1 (\textbf{x}, \boldsymbol{\omega}_o, \sigma) &= \textrm{LERF} (\{\textbf{L}_s^1 (\textbf{x}, \boldsymbol{\omega}_o, \sigma_i) \}_{i=1}^6, \sigma) \\
\textbf{L}_s^0 (\textbf{x}, \boldsymbol{\omega}_o, \sigma_i) &= \int_{\Omega_{+}} \textbf{L}_i (\textbf{x}, \boldsymbol{\omega}_i) \frac{F_0 DG}{4 (\textbf{n} \cdot \boldsymbol{\omega}_o)} d \boldsymbol{\omega}_i \\
\textbf{L}_s^1 (\textbf{x}, \boldsymbol{\omega}_o, \sigma_i) &= \int_{\Omega_{+}} \textbf{L}_i (\textbf{x}, \boldsymbol{\omega}_i) \frac{F_1 DG}{4 (\textbf{n} \cdot \boldsymbol{\omega}_o)} d \boldsymbol{\omega}_i \\
F_0 &= 1 - (1 - \textbf{h} \cdot \boldsymbol{\omega}_i)^5, \quad F_1 = (1 - \textbf{h} \cdot \boldsymbol{\omega}_i)^5
\end{aligned}
$$

## Inverse Rendering from LDR Images
<center><img src='{{"/assets/img/iris/iris-fig3.webp" | relative_url}}' width="100%"></center>
<br>
본 논문의 목표는 exposure level $$\{\Delta t_i\}$$로 촬영된 멀티뷰 LDR 이미지 $$\{\textbf{I}_i\}$$가 주어지면, 공간적으로 변화하는 HDR 조명, 표면 material, 그리고 모든 입력 이미지에서 공유되는 CRF를 추정하는 것이다. 입력 이미지에서 주요 광원이 관측된다고 가정한다.

기존 접근법은 material 및 조명 최적화 과정에서 미분 가능한 path tracing을 수행하지만, 과도한 계산으로 인해 샘플 수가 제한적이다. 이는 높은 분산과 불안정한 추정을 초래한다. Factorized light transport는 BRDF 파라미터와 shading을 분할하고 이를 번갈아 업데이트할 수 있도록 한다. 이 방법은 더 안정적이고 고품질 분해 결과를 생성하는 것으로 나타났지만, shading map을 집계 및 backing하고, emitter를 감지하고, HDR emission을 추정하기 위해 HDR 이미지를 입력으로 사용해야 한다.

따라서 저자들은 조명, material, CRF를 교대로 추정하는 다단계 최적화 전략을 제안하였다. 

1. Monocular inverse rendering 기법을 사용하여 BRDF를 초기화
2. 물리 기반 렌더링(PBR)을 사용하여 LDR에서 HDR 조명을 복원
3. Diffuse shading과 specular shading을 backing
4. BRDF와 CRF를 공동으로 추정

모든 추정값이 수렴할 때까지 2~4단계를 순서대로 반복한다. 

##### Geometry reconstruction
카메라 포즈가 포함된 입력 이미지에서 [BakedSDF](https://arxiv.org/abs/2302.14859)를 사용하여 geometry를 재구성하고, [Omnidata](https://arxiv.org/abs/2110.04994)의 normal 추정 결과를 사용한다.

### 1. BRDF Initialization
각 입력 이미지 $$\textbf{I}_i$$에 대해 단일 이미지 albedo 추정 모델을 사용하고 각 semantic part 내에서 픽셀별 albedo 값을 평균하여 2D albedo map $$\textbf{A}_i$$를 예측한다. 주어진 $$\{\textbf{A}_i\}$$에 대해 모든 입력 이미지에 대해 projection된 albedo와 추정된 albedo 간의 오차를 최소화하여 알베도 $\textbf{a}(\textbf{x})$를 초기화한다. 

$$
\begin{equation}
\min_\textbf{a} \sum_i \| \pi (\textbf{a}) - \textbf{A}_i \|_2
\end{equation}
$$

($$\pi_i$$는 perspective projection)

합성 장면의 경우 part ID에서 semantic 카테고리 레이블을 얻고, 현실 장면의 경우 [Mask2Former](mask2former)를 사용한다. Roughness와 metallicity는 모두 0.5로 초기화된다.

### 2. HDR Emission Restoration
<center><img src='{{"/assets/img/iris/iris-fig4.webp" | relative_url}}' width="68%"></center>
<br>
Emitter가 뷰 전체에서 밝게 나타나고 LDR 이미지에서 채도가 높은 픽셀을 생성한다는 점을 감안할 때, 표면 메쉬에서 binary emitter geometry $$M_e (\textbf{x})$$를 다음과 같이 추정한다.

$$
\begin{equation}
M_e (\textbf{x}) = \begin{cases} 1 & \textrm{if} \; \frac{1}{N} \sum_i v_i (\textbf{x}) \textbf{I}_i (\pi_i (\textbf{x})) \ge 0.99 \\ 0 & \textrm{otherwise} \end{cases}
\end{equation}
$$

($$v (\cdot) \in \{0, 1\}$$는 visibility)

HDR intensity는 inverse rendering에 필수적이지만 LDR 이미지에는 없다. HDR 조명을 복원하기 위해 single-bounce path tracing을 이용한 PBR을 수행하여 이미지를 생성한다. 카메라 광선에서 관측된 포인트 $\textbf{x}$의 radiance를 추정하기 위해, 현재 BRDF 추정값으로 샘플링된 $\textbf{x}$에서 $N = 128$개의 secondary ray를 추적한다. 결과 radiance는 다음과 같이 근사된다.

$$
\begin{aligned}
\textbf{L}_r (\textbf{x}, \boldsymbol{\omega}_o) &= \sum_{i=1}^N \textbf{L}_\textrm{end} (\textbf{x}^\prime) (\boldsymbol{\omega}_i, \boldsymbol{\omega}_o) \\
\textbf{L}_\textrm{end} (\textbf{x}) &= \begin{cases} \textbf{L}_e (\textbf{x}) & \textrm{if} \; M_e (\textbf{x}) = 1 \\ \textbf{L}_\textrm{SLF} (\textbf{x}) & \textrm{otherwise} \end{cases}
\end{aligned}
$$

($$\textbf{x}^\prime = \textbf{x} + d \cdot \boldsymbol{\omega}_i$$는 $$\boldsymbol{\omega}_i$$ 방향을 따라 secondary ray가 교차하는 지점)

직관적으로, secondary ray가 emitter에 직접 닿으면 학습 가능한 emission radiance $$\textbf{L}_e (\textbf{x})$$에서 radiance를 구하고, 그렇지 않으면 미리 계산된 SLF $$\textbf{L}_\textrm{SLF}(\textbf{x})$$에서 global illumination을 근사한다.

<center><img src='{{"/assets/img/iris/iris-fig5.webp" | relative_url}}' width="63%"></center>
<br>
HDR emitter radiance를 복구하기 위해 gradient descent를 사용하여 photometric loss를 최소화한다.

$$
\begin{equation}
\mathcal{L}_\textrm{photo} = \sum_i \| \textrm{CRF} (\min (\textbf{L}_o (\textbf{x}, \boldsymbol{\omega}_o) \Delta t_i, 1)) - \textbf{I}_i \|_2
\end{equation}
$$

Emitter radiance $$\textbf{L}_e (\textbf{x})$$는 촬영된 이미지와 일치하도록 향상되고, BRDF와 CRF는 이 단계에서 고정된다. 직관적으로, PBR에서 $$\mathcal{L}_\textrm{photo}$$를 최소화하려면 emitter radiance $$\textbf{L}_e (\textbf{x})$$를 크게 증가시켜 렌더링된 이미지의 전반적인 밝기를 향상시켜야 한다. 따라서 이 과정에서 HDR 조명이 복원된다.

### 3. Shading Baking
HDR emission이 복구된 후, factorized light transport에 따라 diffuse shading map $$\textbf{L}_d$$와 specular shading map $$\textbf{L}_s^0$$, $$\textbf{L}_s^1$$를 backing한다. 카메라 광선이 교차하는 표면 지점이 주어지면, 고정된 현재 BRDF에 따라 여러 광선이 샘플링된다. Incident radiance는 multi-bounce path tracing을 사용하여 계산된다. 여기서 광선이 emitter에 닿으면 추적을 멈추고, 그렇지 않으면 추적을 계속한다.

### 4. BRDF & CRF Optimization
Backing된 shading map $$\textbf{L}_d$$, $$\textbf{L}_s^0$$, $$\textbf{L}_s^1$$를 사용하여 albedo, metallicity, roughness, CRF가 공동으로 최적화된다.

$$
\begin{equation}
\min_{\textbf{a}, m, \sigma, \textbf{g}} \mathcal{L}_\textrm{photo} + \lambda_a \mathcal{L}_\textrm{albedo} + \lambda_c \mathcal{L}_\textrm{CRF} + \lambda_m \mathcal{L}_\textrm{mat}
\end{equation}
$$

($$\lambda_a = 0.01$$, $$\lambda_c = 0.001$$, $$\lambda_m = 0.005$$)

##### Albedo regularization
[MonoSDF](https://arxiv.org/abs/2206.00665)의 shift-scale-invariant loss와 비슷하게, monocular albedo map $$\{\textbf{A}_i\}에 대한 shift-invariant loss를 채택한다.

$$
\begin{equation}
\mathcal{L}_\textrm{albedo} = \sum_i \| \pi_i (\textbf{a}) - s_i \textbf{A}_i \|_2
\end{equation}
$$

($s_i$는 두 albedo map을 정렬하는 데 사용되는 scale)

##### CRF regularization
학습된 CRF가 평균 곡선 $\bar{\textbf{g}}$에서 크게 벗어나지 않아야 하고 곡선이 단조롭게 증가해야 한다는 가정에 따라 추정된 PCA basis들의 계수의 $L_2$ norm을 정규화하고 샘플링된 CRF의 단조성을 다음과 같이 강제한다.

$$
\begin{equation}
\mathcal{L}_\textrm{CRF} = \| \textbf{w} \|_2 + \sum_i \max (g_{i-1} - g_i, 0)
\end{equation}
$$

##### Material regularization
Roughness-metallicity 정규화 $$\mathcal{L}_\textrm{mat}$$를 적용하여 동일한 semantic 인스턴스 내에서 표면 roughness와 metallicity의 일관성을 강화한다. Shading과 emission이 처음에는 부정확하여 잘못된 BRDF를 생성하지만, BRDF를 번갈아 업데이트하여 emission과 shading을 개선하고 BRDF를 향상시킨다. 이러한 번갈아 업데이트하는 전략은 더욱 안정적인 최적화와 더 나은 추정을 제공한다.

## Experiments
### 1. Inverse Rendering of Real Scenes
다음은 현실 장면에 대한 inverse rendering 결과들이다.

<center><img src='{{"/assets/img/iris/iris-fig6.webp" | relative_url}}' width="100%"></center>

### 2. View Synthesis and Relighting of Real Scenes
다음은 현실 장면에 대한 relighting 및 물체 삽입 결과들이다.

<center><img src='{{"/assets/img/iris/iris-fig7.webp" | relative_url}}' width="100%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/iris/iris-fig8.webp" | relative_url}}' width="85%"></center>

### 3. Quantitative Evaluation
다음은 합성 장면에 대한 BRDF와 emission 비교 결과이다.

<center><img src='{{"/assets/img/iris/iris-table1.webp" | relative_url}}' width="42%"></center>
<br>
다음은 novel view synthesis (NVS)와 relighting 성능을 비교한 결과이다.

<center><img src='{{"/assets/img/iris/iris-table2.webp" | relative_url}}' width="45%"></center>

### 4. Ablation Study
다음은 ablation study 결과이다.

<center><img src='{{"/assets/img/iris/iris-table3.webp" | relative_url}}' width="57%"></center>

### 5. CRF Estimation
다음은 CRF 추정 결과이다.

<center><img src='{{"/assets/img/iris/iris-fig9.webp" | relative_url}}' width="90%"></center>