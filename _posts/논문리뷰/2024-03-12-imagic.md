---
title: "[논문리뷰] Imagic: Text-Based Real Image Editing with Diffusion Models"
last_modified_at: 2024-03-12
categories:
  - 논문리뷰
tags:
  - Diffusion
  - Image Editing
  - Computer Vision
  - AI
  - Google
excerpt: "Imagic 논문 리뷰 (CVPR 2023)"
use_math: true
classes: wide
---

> CVPR 2023. [[Paper](https://arxiv.org/abs/2210.09276)] [[Page](https://imagic-editing.github.io/)] [[Diffusers](https://github.com/huggingface/diffusers/tree/main/examples/community#imagic-stable-diffusion)]  
> Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani  
> Google Research | Technion | Weizmann Institute of Science  
> 17 Oct 2022  

<center><img src='{{"/assets/img/imagic/imagic-fig1.PNG" | relative_url}}' width="100%"></center>

## Introduction
실제 사진에 의미론적 편집을 적용하는 것은 오랫동안 이미지 처리에서 흥미로운 task이었다. 텍스트 기반 이미지 편집을 위한 많은 방법이 개발되어 유망한 결과를 보여주고 지속적으로 개선되고 있다. 그러나 현재의 주요 방법은 몇 가지 단점을 안고 있다. 

1. 이미지 위에 페인팅, 물체 추가, 스타일 전송과 같은 특정 편집으로 제한된다. 
2. 특정 도메인의 이미지나 합성으로 생성된 이미지에서만 작동할 수 있다. 
3. 입력 이미지 외에 원하는 편집 위치를 나타내는 이미지 마스크, 동일한 피사체의 여러 이미지 또는 원본 이미지를 설명하는 텍스트와 같은 보조 입력이 필요하다.

본 논문에서는 위와 같은 문제점을 모두 완화하는 의미론적 이미지 편집 방법을 제안하였다. 편집할 입력 이미지와 편집을 설명하는 텍스트 프롬프트 하나만 주어지면 본 논문의 방법은 실제 고해상도 이미지에 대해 정교한 편집을 수행할 수 있다. 결과 이미지는 원본 이미지의 전체 배경, 구조 및 구성을 유지하면서 타겟 텍스트와 잘 일치한다. **Imagic**이라고 부르는 본 논문의 방법은 여러 물체의 편집을 포함하여 하나의 실제 고해상도 이미지에 이러한 정교한 조작을 적용하는 텍스트 기반 의미론적 편집의 첫 번째 방법이다. 또한 Imagic은 스타일 변경, 색상 변경, 물체 추가 등 다양한 편집 작업도 수행할 수 있다.

저자들은 text-to-image diffusion model의 최근 성공을 활용하였다. Diffusion model은 고품질 이미지 합성이 가능한 강력한 SOTA 생성 모델이다. 자연어 텍스트 프롬프트를 조건으로 하면 요청된 텍스트와 잘 일치하는 이미지를 생성할 수 있다. 새로운 이미지를 합성하는 대신 실제 이미지를 편집하기 위해 diffusion model을 적용한다. 간단한 3단계 프로세스로 이를 수행한다. 먼저 텍스트 임베딩을 최적화하여 입력 이미지와 유사한 이미지를 생성한다. 그런 다음 사전 학습된 diffusion model을 fine-tuning하여 입력 이미지를 더 잘 재구성한다. 마지막으로 타겟 텍스트 임베딩과 최적화된 텍스트 사이를 linear interpolation하여 입력 이미지와 타겟 텍스트를 모두 결합한 표현을 얻는다. 그런 다음 이 표현은 fine-tuning된 모델을 사용하여 생성 프로세스로 전달되어 최종 편집된 이미지를 출력한다. 

Imagic은 입력 이미지와 매우 유사하고 타겟 텍스트와 잘 어울리는 고품질 이미지를 출력한다. 이러한 결과는 Imagic의 일반성, 다양성, 품질을 보여준다. Imagic은 특히 정교하고 비정형(non-rigid) 편집을 수행할 때 훨씬 더 나은 편집 품질과 원본 이미지에 대한 충실도를 보여준다. 

## Imagic: Diffusion-Based Real Image Editing
입력 이미지 $x$와 원하는 편집 내용을 설명하는 대상 텍스트가 주어지면, 본 논문의 목표는 $x$의 디테일을 최대한 유지하면서 주어진 텍스트를 만족시키는 방식으로 이미지를 편집하는 것이다. 이를 위해 diffusion model의 텍스트 임베딩 레이어를 활용하여 의미론적(semantic) 조작을 수행한다. GAN 기반 접근 방식과 마찬가지로 생성 프로세스를 통해 입력 이미지와 유사한 이미지를 생성하는 의미 있는 표현을 찾는 것부터 시작한다. 그런 다음 생성 모델을 fine-tuning하여 입력 이미지를 더 잘 재구성하고 마지막으로 latent 표현을 조작하여 편집 결과를 얻는다.

<center><img src='{{"/assets/img/imagic/imagic-fig3.PNG" | relative_url}}' width="100%"></center>
<br>
위 그림에 설명된 대로 본 논문의 방법은 3단계로 구성된다. 

1. 텍스트 임베딩을 최적화하여 타겟 텍스트 임베딩 근처에서 주어진 이미지와 가장 잘 일치하는 이미지를 찾는다. 
2. 주어진 이미지와 더 잘 일치하도록 diffusion model을 fine-tuning한다. 
3. 입력 이미지와 타겟 텍스트 정렬에 대한 충실도를 모두 달성하는 지점을 찾기 위해 최적화된 임베딩과 타겟 텍스트 임베딩 사이를 선형적으로 interpolation한다. 

#### 텍스트 임베딩 최적화
타겟 텍스트는 먼저 텍스트 인코더에 전달되어 텍스트 임베딩 $$\textbf{e}_\textrm{tgt} \in \mathbb{R}^{T \times d}$$를 출력한다. 여기서 $T$는 주어진 타겟 텍스트의 토큰 수이고 $d$는 토큰 임베딩 차원이다. 그런 다음 diffusion model $f_\theta$의 파라미터를 동결하고 diffusion loss function를 사용하여 타겟 텍스트 임베딩 $$\textbf{e}_\textrm{tgt}$$를 최적화한다.

$$
\begin{equation}
\mathcal{L}(\textbf{x}, \textbf{e}, \theta) = \mathbb{E}_{t, \boldsymbol{\epsilon}} [\| \boldsymbol{\epsilon} - f_\theta (\textbf{x}_t, t, \textbf{e})]
\end{equation}
$$

여기서 $t \sim \textrm{Uniformr}[1,T]$이고, $$\textbf{x}_t$$는 $$\boldsymbol{\epsilon} \sim \mathcal{N}(0,\textbf{I})$$를 사용하여 얻은 입력 이미지 $\textbf{x}$의 noisy 버전이고 $\theta$는 사전 학습된 diffusion model의 가중치이다. 그 결과 입력 이미지와 최대한 일치하는 텍스트 임베딩이 생성된다. 초기 타겟 텍스트 임베딩에 가깝게 유지된 $$\textbf{e}_\textrm{opt}$$를 얻기 위해 비교적 적은 step으로 이 프로세스를 실행한다. 이러한 근접성은 임베딩 공간에서 의미 있는 linear interpolation을 가능하게 한다. 

#### 모델 fine-tuning
최적화된 임베딩 $$\textbf{e}_\textrm{opt}$$가 반드시 생성 프로세스를 통과할 때 입력 이미지 $\textbf{x}$로 이어지는 것은 아니다. 최적화는 적은 수의 step에 대해 실행되기 때문이다. 따라서 두 번째 단계에서는 최적화된 임베딩을 고정하면서 동일한 loss function을 사용하여 모델 파라미터 $\theta$를 최적화하여 이 격차를 해소한다. 이 프로세스는 $$\textbf{e}_\textrm{opt}$$에서 입력 이미지 $\mathbf{x}$에 맞게 모델을 이동시킨다. 이와 동시에 super-resolution(SR) 모델과 같은 모든 보조 diffusion model을 fine-tuning한다. 동일한 재구성 loss로 fine-tuning하지만 편집된 이미지에서 작동하므로 $$\textbf{e}_\textrm{tgt}$$로 컨디셔닝된다. 이러한 보조 모델의 최적화는 저해상도에 존재하지 않는 $\mathbf{x}$의 고주파수 디테일을 보존하는 것을 보장한다. 저자들은 경험적으로 inference 시 $$\textbf{e}_\textrm{tgt}$$를 보조 모델에 입력하는 것이 $$\textbf{e}_\textrm{opt}$$를 사용하는 것보다 더 나은 성능을 발휘한다는 것을 발견했다.

#### 텍스트 임베딩 interpolation
Diffusion model은 최적화된 임베딩 $$\textbf{e}_\textrm{opt}$$에서 입력 이미지 $\mathbf{x}$를 완전히 재현하도록 학습되었으므로 이를 사용하여 타겟 텍스트 임베딩 $$\textbf{e}_\textrm{tgt}$$ 방향으로 진행하여 원하는 편집을 적용한다. 즉, 세 번째 단계는 $$\textbf{e}_\textrm{tgt}$$와 $$\textbf{e}_\textrm{opt}$$ 사이의 간단한 linear interpolation이다. 주어진 하이퍼파라미터 $\eta \in [0,1]$에 대해 원하는 편집된 이미지를 나타내는 임베딩 $\bar{\mathbf{e}}$는 다음과 같다.

$$
\begin{equation}
\bar{\mathbf{e}} = \eta \cdot \mathbf{e}_\textrm{tgt} + (1 - \eta) \cdot \mathbf{e}_\textrm{opt}
\end{equation}
$$

그런 다음 $\bar{\mathbf{e}}$에 따라 조정된 fine-tuning된 모델을 사용하여 생성 프로세스를 적용한다. 그 결과 저해상도 편집 이미지가 생성되고, 타겟 텍스트에 맞춰 fine-tuning된 보조 모델을 사용하여 super-resolution(SR) 이미지가 생성된다. 이 생성 프로세스는 최종 고해상도 편집 이미지 $\bar{\mathbf{x}}$를 출력한다.

## Experiments
- 구현 디테일
  - Imagen
    - optimizer: Adam (learning rate: $10^{-3}$)
    - 텍스트 임베딩 최적화: 100 steps
    - fine-tuning
      - 64$\times$64: 1500 steps
      - 64$\times$64 $\rightarrow$ 256$\times$256: 1500 steps
      - 256$\times$256 $\rightarrow$ 1024$\times$1024: fine-tuning하지 않음 (효과 미미)
    - 전체 프로세스는 2개의 TPUv4 칩에서 이미지당 8분 소요
  - [Stable Diffusion](https://kimjy99.github.io/논문리뷰/ldm)
    - optimizer: Adam
    - 텍스트 임베딩 최적화: 1000 steps (learning rate: $2 times 10^{-3}$)
    - fine-tuning: 1500 steps (learning rate: $5 times 10^{-7}$)
    - 전체 프로세스는 1개의 Tesla A100 GPU에서 이미지당 7분 소요
  - 임베딩 interpolation시 DDIM을 사용하는 것이 stochastic한 DDPM보다 결과가 좋음

### 1. Qualitative Evaluation
다음은 여러 타겟 텍스트를 동일한 이미지에 적용한 결과들이다. 

<center><img src='{{"/assets/img/imagic/imagic-fig2.PNG" | relative_url}}' width="100%"></center>
<br>
다음은 여러 랜덤시드로 편집한 결과들이다. 

<center><img src='{{"/assets/img/imagic/imagic-fig4.PNG" | relative_url}}' width="100%"></center>
<br>
다음은 임베딩 interpolation 결과들이다. 

<center><img src='{{"/assets/img/imagic/imagic-fig5.PNG" | relative_url}}' width="100%"></center>

### 2. Comparisons
다음은 여러 이미지 편집 방법들과 비교한 결과이다. 

<center><img src='{{"/assets/img/imagic/imagic-fig6.PNG" | relative_url}}' width="100%"></center>

### 3. User Study
다음은 입력 이미지 및 복잡한 편집을 설명한 타겟 텍스트 100 쌍에 대한 user study 결과이다. 

<center><img src='{{"/assets/img/imagic/imagic-fig8.PNG" | relative_url}}' width="75%"></center>

### 4. Ablation Study
다음은 임베딩 interpolation 시 사전 학습된 모델(위)과 fine-tuning된 모델(아래)의 결과를 동일한 랜덤시드에서 비교한 결과이다. 

<center><img src='{{"/assets/img/imagic/imagic-fig7.PNG" | relative_url}}' width="100%"></center>
<br>
다음은 $\eta$에 따른 텍스트 정렬(CLIP score)과 이미지 품질(1 - LPIPS)의 trade-off를 나타낸 그래프이다. 

<center><img src='{{"/assets/img/imagic/imagic-fig9.PNG" | relative_url}}' width="50%"></center>

## Limitations
<center><img src='{{"/assets/img/imagic/imagic-fig10.PNG" | relative_url}}' width="100%"></center>
<br>
Imagic에는 두 가지 주요 실패 사례가 있다. 

1. 원하는 편집이 매우 미묘하게 적용되어 타겟 텍스트와 잘 정렬되지 않는다. 
2. 편집 내용이 잘 적용되지만 줌이나 카메라 각도와 같은 외부 이미지 디테일에 영향을 미친다. 

편집 내용이 충분히 강력하게 적용되지 않은 경우 일반적으로 $\eta$를 높이면 원하는 결과를 얻을 수 있지만 일부 경우에는 원본 이미지 디테일이 크게 손실되는 경우가 있다. 줌 및 카메라 각도 변경은 일반적으로 원하는 편집이 이루어지기 전에 발생한다. 낮은 $\eta$ 값에서 큰 값으로 진행하므로 이를 회피하기가 어렵다.

또한 Imagic은 사전 학습된 text-to-image diffusion model에 의존하기 때문에 모델의 생성적 한계와 편향을 상속한다. 따라서 원하는 편집에 기본 모델의 실패 사례 생성이 포함되면 원치 않는 아티팩트가 생성된다. 또한 Imagic에 필요한 최적화는 느리며 사용자 대상 애플리케이션에 직접 배포하는 데 방해가 될 수 있다. 