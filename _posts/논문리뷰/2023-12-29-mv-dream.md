---
title: "[논문리뷰] MVDream: Multi-view Diffusion for 3D Generation"
last_modified_at: 2023-12-29
categories:
  - 논문리뷰
tags:
  - Diffusion
  - Text-to-3D
  - 3D Vision
  - AI
excerpt: "MVDream 논문 리뷰"
use_math: true
classes: wide
---

> arXiv 2023. [[Paper](https://arxiv.org/abs/2308.16512)] [[Page](https://mv-dream.github.io/)] [[Github](https://github.com/bytedance/MVDream)]  
> Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, Xiao Yang  
> ByteDance | University of California, San Diego  
> 31 Aug 2023  

## Introduction
기존 3D 객체 생성 방법은 템플릿 기반 생성 파이프라인, 3D 생성 모델, 2D 리프팅 방법의 세 가지 유형으로 분류할 수 있다. 제한된 3D 모델과 대규모 데이터 복잡성으로 인해 템플릿 기반 생성기와 3D 생성 모델 모두 임의의 객체 생성에 효과적으로 일반화하는 데 어려움을 겪는다. 생성된 콘텐츠는 상대적으로 단순한 토폴로지와 텍스처를 일반적인 객체로 제한되는 경우가 많다. 그러나 인기 있는 3D 에셋은 일반적으로 복잡하고 예술적이며 때로는 비현실적인 구조와 스타일이 혼합된 형태로 제공된다. 

최근 2D 리프팅 방법은 사전 학습된 2D 생성 모델이 3D 생성에 잠재적으로 적용될 수 있음을 보여주었다. 일반적인 표현은 [Dreamfusion](https://kimjy99.github.io/논문리뷰/dreamfusion)과 [Magic3D](https://arxiv.org/abs/2211.10440) 시스템으로, Score Distillation Sampling (SDS)을 통한 3D 표현 최적화를 위한 supervision으로 2D diffusion model을 활용한다. 대규모 2D 이미지 데이터셋으로 학습된 2D 모델은 텍스트 입력을 통해 디테일을 지정할 수 있어 보이지 않는 반사실적 장면을 생성할 수 있으므로 에셋을 생성하는 데 훌륭한 도구가 된다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig1.PNG" | relative_url}}' width="100%"></center>
<br>
그럼에도 불구하고 2D 리프팅 기술에서는 score distillation 중 포괄적인 멀티뷰 지식이나 3D 인식이 부족하여 문제가 발생한다. 이러한 과제는 다음을 포함한다. 

1. Multi-face Janus problem
2. Content Drift Problem 

Janus problem은 다양한 요인으로 인해 발생할 수 있다. 예를 들어, 칼날과 같은 특정 물체는 어떤 각도에서는 거의 보이지 않을 수 있다. 한편, 캐릭터나 동물의 중요한 부분은 특정 시점에서 숨겨져 있거나 자체적으로 가려질 수 있다. 인간은 이러한 객체를 여러 각도에서 평가하지만 2D diffusion model은 그렇게 할 수 없어 중복되고 일관되지 않은 콘텐츠를 생성하게 된다. 

2D 리프팅 방법의 이러한 약점에도 불구하고 일반화 가능한 3D 생성을 위해서는 대규모 2D 데이터가 중요하다. 따라서 본 논문은 3D 표현에 독립적인 멀티뷰 3D prior로 사용될 수 있는 멀티뷰 diffusion model을 제안하였다. 제안된 모델은 서로 일치하는 멀티뷰 이미지 집합을 동시에 생성한다. 일반화 능력을 상속하기 위해 transfer learning을 위해 사전 학습된 2D diffusion model을 활용할 수 있다. 그런 다음 멀티뷰 이미지와 2D 이미지-텍스트 쌍에 대한 모델을 공동으로 학습함으로써 우수한 일관성과 일반화 능력을 모두 달성할 수 있다. Score distillation을 통해 3D 생성에 적용하면 멀티뷰 supervision이 단일 뷰 2D diffusion model보다 훨씬 더 안정적이다. 그리고 순수한 2D diffusion model처럼 눈에 보이지 않는 반사실적인 3D 콘텐츠를 만들 수도 있다. [DreamBooth](https://kimjy99.github.io/논문리뷰/dreambooth)에서 영감을 받아 멀티뷰 diffusion model을 사용하여 2D 이미지 컬렉션의 identity를 받아들일 수도 있으며 fine-tuning 후 강력한 멀티뷰 일관성을 보여준다. 전반적으로 본 논문의 모델, 즉 **MVDream**은 멀티뷰 일관성 문제 없이 3D Nerf 모델을 성공적으로 생성한다. 이는 다른 SOTA 방법에서 볼 수 있는 다양성을 능가한다.

## Methodology
### 1. Multi-view Diffusion Model
2D 리프팅 방법의 멀티뷰 일관성 문제를 완화하기 위한 일반적인 솔루션은 시점 인식을 향상시키는 것이다. 보다 정교한 방법은 새로운 뷰 합성 방법과 같이 정확한 카메라 파라미터를 통합하는 것이다. 그러나 완벽한 카메라 조건부 모델조차도 문제를 해결하는 데 충분하지 않으며 다양한 뷰의 콘텐츠가 여전히 불일치할 수 있다. 

저자들의 영감은 video diffusion model에서 비롯되었다. 인간에게는 실제 3D 센서가 없기 때문에 3D 물체를 인식하는 일반적인 방법은 회전하는 동영상을 보는 것과 유사하게 가능한 모든 관점에서 관찰하는 것이다. 동영상 생성에 관한 최근 연구에서는 시간적으로 일관된 콘텐츠를 생성하기 위해 이미지 diffusion model을 적용할 수 있는 가능성을 보여주었다. 그러나 이러한 동영상 모델을 본 논문의 문제에 적용하는 것은 기하학적 일관성이 시간적 일관성보다 더 섬세할 수 있기 때문에 쉽지 않다. 시점이 크게 변경될 때 video diffusion model의 프레임 간에 content drift가 계속 발생할 수 있다. 또한 video diffusion model은 일반적으로 동적 장면에 대해 학습되는데, 정적 장면 생성에 적용할 때 도메인 격차가 발생할 수 있다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig2.PNG" | relative_url}}' width="100%"></center>
<br>
이러한 관찰을 통해 저자들은 3D 렌더링 데이터셋을 활용하여 정확한 카메라 파라미터로 정적 장면을 생성할 수 있는 멀티뷰 diffusion model을 직접 학습시키는 것이 중요하다는 것을 알았다. 위 그림은 text-to-multi-view diffusion model을 보여준다. 3D 데이터셋을 활용하여 일관된 멀티뷰 이미지를 렌더링하여 diffusion model을 학습시킨다. 

Noisy한 이미지의 집합 $x_t \in \mathbb{R}^{F \times H \times W \times C}$, 텍스트 프롬프트 $y$, 외부 카메라 파라미터 집합 $c \in \mathbb{R}^{F \times 16}$이 주어지면 멀티뷰 diffusion model은 $F$개의 다른 시야각에서 동일한 장면의 이미지의 집합 $x_0 \in \mathbb{R}^{F \times H \times W \times C}$를 생성하도록 학습된다. 학습 후 Score Distillation Sampling (SDS)와 같은 기술을 사용하여 모델을 3D 생성을 위한 멀티뷰로 사용할 수 있다. 

2D diffusion model의 일반화 능력을 계승하기 위해서는 아키텍처를 최대한 유지하고 fine-tuning을 해야 한다. 그러나 2D diffusion model은 한 번에 하나의 이미지만 생성할 수 있으며 카메라 조건을 입력으로 사용하지 않는다. 따라서 여기서 주요 질문은 다음과 같다. 

1. 어떻게 동일한 텍스트 프롬프트에서 일관된 이미지 집합을 생성하는가
2. 어떻게 카메라 포즈 제어를 추가하는가
3. 어떻게 품질과 일반화 능력을 유지하는가

#### 1.1 Multi-view Consistent Image Generation
Video diffusion model과 유사하게, 나머지 네트워크를 단일 이미지 내에서만 작동하는 2D 모델로 유지하면서 뷰 사이의 의존성을 모델링하기 위해 attention 레이어를 적응시킨다. 그러나 3D 렌더링 데이터셋에서 모델을 fine-tuning하더라도 단순한 temporal attention이 멀티뷰 일관성을 학습하지 못하고 content drift가 여전히 발생한다. 대신에 저자들은 3D attention을 사용하기로 선택했다. Self-attention 레이어의 모든 다른 뷰를 연결하여 원본 2D self-attention 레이어를 3D로 변환할 수 있으며, 뷰 차이가 매우 큰 경우에도 다소 일관된 이미지를 생성할 수 있다. 저자들은 기존 2D 레이어를 수정하는 대신 새로운 3D self-attention 레이어를 통합하는 실험도 진행했다. 그러나 이러한 디자인은 멀티뷰 이미지의 생성 품질을 손상시켰다. 이는 기존 모듈의 파라미터를 재사용하는 것에 비해 새로운 attention 모듈의 수렴 속도가 느린 것이 원인이다. 

#### 1.2 Camera Embeddings
Video diffusion model과 마찬가지로 모델이 서로 다른 뷰를 구별하려면 위치 인코딩이 필요하다. 이를 위해 저자들은 [relative position encoding](https://kimjy99.github.io/논문리뷰/make-a-video), [rotary
embedding](https://arxiv.org/abs/2104.09864), 절대적인 카메라 파라미터를 비교했다. 저자들은 2-layer MLP에 카메라 파라미터를 임베딩하면 눈에 띄는 뷰 차이로 가장 만족스러운 이미지 품질을 얻을 수 있다는 것을 발견했다. 구체적으로 저자들은 카메라 파라미터를 주입하는 두 가지 방법을 고려하였다. 

1. 카메라 임베딩을 residual로 시간 임베딩에 더해줌
2. Cross-attention을 위해 카메라 임베딩을 텍스트 임베딩에 추가

저자들의 실험에 따르면 두 방법 모두 작동하지만 카메라 임베딩이 텍스트와 덜 얽혀 있기 때문에 전자가 더 강력한 것으로 나타났다.

#### 1.3 Training Loss Function
<center><img src='{{"/assets/img/mv-dream/mv-dream-fig3a.PNG" | relative_url}}' width="50%"></center>
<br>
데이터 선별 및 학습 전략의 세부 사항이 이미지 생성 품질에도 중요하다. 학습을 위해 Stable Diffusion v2.1 기본 모델(512$\times$512)에서 모델을 fine-tuning한다. 여기서 최적화 도구와 $\epsilon$-prediction에 대한 설정은 유지하지만 이미지 크기는 256$\times$256으로 줄인다. 위 그림에서 볼 수 있듯이 더 큰 규모의 텍스트-이미지 데이터셋을 사용한 공동 학습이 fine-tuning 모델의 일반화에 도움이 된다다. 텍스트-이미지 데이터셋 $\mathcal{X}$와 멀티뷰 데이터셋 $$\mathcal{X}_\textrm{mv}$$가 주어지면, 학습 샘플 $$\{x, y, c\} \in \mathcal{X} \cup \mathcal{X}_\textrm{mv}$$에 대해 멀티뷰 diffusion loss는 다음과 같이 정의된다.

$$
\begin{equation}
\mathcal{L}_\textrm{MV} (\theta, \mathcal{X}, \mathcal{X}_\textrm{mv}) = \mathbb{E}_{x, y, c, t, \epsilon} [\| \epsilon - \epsilon_\theta (x_t; y, c, t) \|_2^2]
\end{equation}
$$

여기서 $x_t$는 랜덤 noise $\epsilon$과 이미지 $x$에서 생성된 noisy한 이미지이고, $y$는 조건, $c$는 카메라 조건, $\epsilon_\theta$는 멀티뷰 diffusion model이다. 실제로 30% 확률로 3D attention과 카메라 임베딩을 끄고 LAION 데이터셋의 부분 집합에 대한 간단한 2D text-to-image 모델로 멀티뷰 모델을 학습시킨다.

### 2. Text-to-3D Generation
3D 생성을 위해 멀티뷰 diffusion model을 활용하는 두 가지 방법이 있다.

1. 생성된 멀티뷰 이미지를 few-shot 3D 재구성 방법에 대한 입력으로 사용
2. Score Distillation Sampling (SDS)의 prior로 멀티뷰 diffusion model을 사용

3D 재구성은 보다 간단하지만 입력 뷰와 일관성에 대한 요구 사항이 더 높다. 따라서 본 논문에서는 후자에 대한 실험에 중점을 두고 Stable Diffusion 모델을 멀티뷰 diffusion model로 대체하여 기존 SDS 파이프라인을 수정한다. 이로 인해 카메라 샘플링 전략 변경과 카메라 파라미터를 입력으로 제공하는 두 가지 수정이 이루어진다. [Dreamfusion](https://kimjy99.github.io/논문리뷰/dreamfusion)에서와 같이 방향 주석 프롬프트를 사용하는 대신 텍스트 임베딩을 추출하기 위해 원본 프롬프트를 사용한다.

이러한 멀티뷰 SDS가 일관된 3D 모델을 생성할 수 있음에도 불구하고 컨텐츠 풍부함과 텍스처 품질은 여전히 denoising process로 직접 샘플링된 이미지에 비해 부족하다. 따라서 저자들은 이 문제를 완화하기 위한 몇 가지 기술을 제안하였다. 

1. 최적화 중에 SDS의 최대 및 최소 timestep을 선형적으로 어닐링한다. 
2. 모델이 데이터셋에서 낮은 품질의 3D 모델 스타일을 생성하는 것을 방지하기 위해 SDS 중에 몇 가지 고정 negative 프롬프트를 추가한다. 
3. [Classifier free guidance (CFG)](https://kimjy99.github.io/논문리뷰/cfdg)로 인한 색상 saturation을 완화하기 위해 dynamic thresholding 또는 CFG rescale과 같은 클램핑 기술을 적용한다. 

이러한 트릭들은 $$\hat{x}_0$$에만 적용되므로 원래 SDS 공식 대신 $x_0$-reconstruction loss를 사용한다.

$$
\begin{equation}
\mathcal{L}_\textrm{SDS} (\phi, x = g(\phi)) = \mathbb{E}_{t, c, \epsilon} [\| x - \hat{x}_0 \|_2^2]
\end{equation}
$$

위 식은 SDS의 hyperparameter인 $w(t)$가 신호 대 잡음비(SNR)와 동일하여 원본 SDS와 동일함을 알 수 있다. 여기서 $x = g(\phi)$는 3D 표현 $\phi$에서 렌더링된 이미지를 나타내고 $$\hat{x}_0$$는 gradient가 detach된 $\epsilon_\theta (x_t; y, c, t)$에서 추정된 $x_0$이다. $x_0$-reconstruction loss는 원래 SDS와 유사하게 수행되지만 $$\hat{x}_0$$에 CFG rescale 트릭을 적용한 후 색상 saturation을 완화한다. 

또한 geometry를 정규화하기 위해 Dreamfusion의 point lighting과 Magic3d의 soft shading를 사용한다. 정규화 loss의 경우 Dreamfusion이 제안한 orientation loss만 사용한다. 이 두 가지 테크닉은 모두 geometry를 부드럽게 하는 데만 도움이 되며 콘텐츠에는 거의 영향을 미치지 않는다. 전경과 배경을 강제로 분리하기 위해 sparsity loss를 사용하지 않고 대신 배경을 임의의 색상으로 대체한다.

### 3. Multi-view DreamBooth for 3D Generation
<center><img src='{{"/assets/img/mv-dream/mv-dream-fig3b.PNG" | relative_url}}' width="50%"></center>
<br>
위 그림과 같이 멀티뷰 diffusion model을 학습시킨 후 이를 3D DreamBooth 응용을 위한 [DreamBooth](https://kimjy99.github.io/논문리뷰/dreambooth) 모델로 확장할 수 있다. 멀티뷰 diffusion model의 일반화 덕분에 튜닝 후에도 멀티뷰 능력이 유지될 수 있다. 구체적으로, 두 가지 유형의 loss, 즉 이미지 fine-tuning loss와 파라미터 보존 loss를 사용한다. $$\mathcal{X}_\textrm{id}$$를 identity 이미지 집합이라 하면 DreamBooth에 대한 loss는 다음과 같다.

$$
\begin{equation}
\mathcal{L}_\textrm{DB} = \mathcal{L}_\textrm{LDM} (\mathcal{X}_\textrm{id}) + \lambda \frac{\| \theta - \theta_0 \|_1}{N_\theta}
\end{equation}
$$

여기서 $$\mathcal{L}_\textrm{LDM}$$은 이미지 diffusion loss, $\theta_0$는 원래 멀티뷰 diffusion model의 초기 파라미터, $N_\theta$는 파라미터 개수, $\lambda$는 1로 설정된 hyperparameter이다.

3D 모델을 얻기 위해 diffusion model을 DreamBooth 모델로 대체하여 3D 생성 프로세스를 따른다. 원래 [DreamBooth3D](https://arxiv.org/abs/2303.13508)는 DreamBooth, 멀티뷰 데이터 생성, 멀티뷰 DreamBooth의 3단계 최적화를 사용했다. 이에 비해 본 논문의 방법은 diffusion model의 일관성을 활용하고 멀티뷰 DreamBooth 모델(MVDreamBooth)을 학습시킨 후 바로 3D NeRF 최적화를 수행하여 프로세스를 간소화하였다.

## Experiments
### 1. Multi-view Image Generation
다음은 attention 모듈의 영향을 비교한 결과이다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig4.PNG" | relative_url}}' width="90%"></center>
<br>
다음은 이미지 합성 품질을 정량적으로 평가한 표이다. DDIM sampler가 사용되었다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-table1.PNG" | relative_url}}' width="90%"></center>
<br>
다음은 학습 프롬프트와 테스트 프롬프트를 사용하여 모델에서 생성된 예시 이미지들이다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig5.PNG" | relative_url}}' width="90%"></center>

### 2. 3D Generation with Multi-view Score Distillation
다음은 text-to-3D 생성을 다른 모델들과 비교한 것이다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig6.PNG" | relative_url}}' width="90%"></center>
<br>
다음은 멀티뷰 SDS에 적용한 여러 테크닉에 대한 효과를 비교한 것이다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig7.PNG" | relative_url}}' width="100%"></center>
<br>
다음은 38명이 참여한 user study 결과이다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig8.PNG" | relative_url}}' width="45%"></center>

### 3. Multi-view DreamBooth
다음은 MVDreamBooth의 결과이다. 

<center><img src='{{"/assets/img/mv-dream/mv-dream-fig9.PNG" | relative_url}}' width="100%"></center>

## Limitation
1. 현재 모델은 원래 Stable Diffusion의 512$\times$512보다 작은 256$\times$256 해상도에서만 이미지를 생성할 수 있다. 
2. 현재 모델의 일반화 능력은 기본 모델 자체에 의해 제한된다. 
3. 모델의 생성된 스타일(조명, 텍스처)이 렌더링된 데이터셋의 영향을 받는다. 