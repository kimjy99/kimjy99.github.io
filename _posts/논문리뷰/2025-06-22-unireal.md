---
title: "[논문리뷰] UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics"
last_modified_at: 2025-06-22
categories:
  - 논문리뷰
tags:
  - Diffusion
  - Text-to-Image
  - Computer Vision
  - CVPR
excerpt: "UniReal 논문 리뷰 (CVPR 2025)"
use_math: true
classes: wide
---

> CVPR 2025. [[Paper](https://arxiv.org/abs/2412.07774)] [[Page](https://xavierchen34.github.io/UniReal-Page/)]  
> Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, Hengshuang Zhao  
> The University of Hong Kong | Adobe  
> 10 Dec 2024  

<center><img src='{{"/assets/img/unireal/unireal-fig1.webp" | relative_url}}' width="100%"></center>

## Introduction
다양한 이미지 생성 및 편집 task들은 입력 이미지와 출력 이미지 간의 일관성을 유지하면서도 제어된 시각적 변화를 구현해야 하는 공통된 요구 사항을 가지고 있다. [Sora](https://kimjy99.github.io/논문리뷰/sora)와 같은 동영상 생성 모델은 프레임 일관성과 모션 변화의 균형을 효과적으로 유지하며, 이는 본 논문의 목표와 밀접하게 부합한다. 

따라서 본 논문은 이러한 디자인 원칙을 이미지 레벨 task에 적용하여 다양한 이미지 생성 및 편집 task를 불연속적인 프레임 생성을 위한 통합 프레임워크로 재구성하였다. 특히, 하나의 [diffusion transformer](https://kimjy99.github.io/논문리뷰/dit) 내에서 다양한 task를 처리하는 범용 솔루션인 **UniReal**을 소개하였다. UniReal은 동영상 생성 모델의 기본 구조를 기반으로 프레임 간 모델 관계에 대한 full attention을 사용한다. 

UniReal은 다양한 개수의 입력/출력 이미지를 프레임으로 처리하고, 텍스트 프롬프트에 따라 다양한 애플리케이션을 지원한다. 저자들은 여러 task에서 입력 이미지 유형을 통합하기 위해 입력 이미지를 세 가지 축으로 구분하였다. 

1. 편집할 대상 이미지
2. 삽입 또는 보존할 물체 또는 시각적 요소를 포함하는 레퍼런스 이미지
3. 레이아웃 또는 모양 정규화를 위한 조건 맵

통합된 텍스트 프롬프트 아래에 여러 입력 이미지를 통합하기 위해, 저자들은 각 이미지를 해당 프롬프트에 연결하는 이미지 인덱스 임베딩을 도입하였다. 또한 task 시너지를 위해, 기본 프롬프트에 컨텍스트 레벨 및 이미지 레벨 guidance를 계층화하는 계층적 프롬프트 체계를 설계하였다. 이를 통해 UniReal은 다양한 task를 원활하게 통합할 수 있다. 

저자들은 task별 데이터를 모으는 대신, 보편적인 supervision을 사용하고자 하였다. 그 결과, 두 개의 불연속적인 동영상 프레임 사이에서 다양한 유형의 변형이 자연스럽게 처리되는 것을 확인했다. 이런 방식으로 대규모 프레임 쌍을 유익한 편집 데이터로 활용하고, 이미지 커스터마이징 및 이미지 합성 task를 위해 동영상에서 데이터를 구성하는 자동 파이프라인을 구축하였다.

UniReal은 입력 이미지와의 일관성을 유지하고 디테일을 보존하는 데 있어 탁월한 성능을 보여준다. 또한, 자연스러운 변화를 모델링하고 조명, 반사, 물체 상호작용과 같은 실제 역학을 시뮬레이션하는 데에도 강력한 잠재력을 보여주었다. 

## Method
### 1. Model Design
<center><img src='{{"/assets/img/unireal/unireal-fig2.webp" | relative_url}}' width="100%"></center>

##### Diffusion transformer
UniReal은 입력/출력 이미지를 동영상 프레임으로 처리하고 프롬프트를 사용하여 다양한 task를 관리한다. 구체적으로, 이미지는 VAE 인코더에 의해 latent space에 인코딩된 다음, latent map이 생성된다. 비주얼 토큰에 인덱스 임베딩을 더하여 이미지 순서를 구분하고, 이미지 프롬프트를 추가하여 이미지가 전경/배경 (canvas/asset) 물체 역할을 하는지 여부를 나타내었다. 각 이미지/noise 토큰에 위치 임베딩을 더하고, noise 토큰에 timestep 임베딩을 더한다. 동시에 텍스트 프롬프트는 T5 인코더로 전송되어 텍스트 토큰이 추출된다. 이미지/noise 토큰을 텍스트 토큰과 함께 긴 1차원 텐서로 concat하여 transformer에 입력한다. Transformer는 full attention을 사용하여 이미지와 텍스트 프롬프트 간의 관계를 모델링한다.

##### 텍스트-이미지 연결
텍스트 프롬프트에서 특정 이미지를 참조하기 위해, 저자들은 비주얼 토큰과 해당 텍스트를 연결하는 임베딩 쌍 세트를 구성하였다. 구체적으로, "IMG1"과 "IMG2"와 같은 참조 단어를 사용하여 입력 이미지에 사용하고, "RES1"과 "RES2"와 같은 참조 단어를 출력 이미지에 사용하였다. 이 단어들을 T5 tokenizer를 위한 특수 토큰으로 추가된다. 동시에, 각 참조 단어에 대한 이미지 인덱스 임베딩을 학습시켜 해당 이미지의 토큰에 더한다.

##### 계층적 프롬프트
Task/데이터셋마다 동일한 입력을 처리하는 방식이 다르다. 예를 들어, 이미지 편집은 입력 이미지의 레이아웃을 유지하고 로컬한 변경만 수행한다. 그러나 동일한 프롬프트를 사용하더라도 이미지 커스터마이징은 새로운 시나리오를 생성하고 레퍼런스 물체만 유지한다. 이는 학습 및 inference 단계 모두에서 모호성을 야기한다. 

저자들은 여러 task와 데이터 소스를 혼합할 때 발생하는 모호성을 줄이기 위해 계층적 프롬프트를 제안하였다. 기본 프롬프트 외에도, 자세한 정보를 제공하기 위해 추가적인 컨텍스트 프롬프트와 이미지 프롬프트를 설계하였다.

컨텍스트 프롬프트는 "realistic/synthetic data", "static/dynamic senario", "with reference object" 등 다양한 task 및 데이터 소스에 대한 속성 태그를 제공한다. 이전 연구들에서 사용된 task 임베딩과는 달리, 일부 키워드는 task 간에 공유될 수 있어 해당 task들이 공통적인 특징을 학습하도록 한다. 또한, 텍스트는 본질적으로 합칠 수 있기 때문에 다양한 컨텍스트 프롬프트를 쉽게 합쳐 새로운 기능을 구현할 수 있다.

이미지 프롬프트는 입력 이미지의 구체적인 역할을 나타낸다. 입력 이미지를 canvas image, asset image, control image의 세 가지 카테고리로 구분한다. 

- **Canvas image**: 고정된 레이아웃을 가진 편집 대상의 배경 역할을 한다. 
- **Asset image**: 이미지 커스터마이징 또는 이미지 합성을 위한 레퍼런스 물체 또는 시각적 요소를 제공하며, 모델은 이를 위해 암시적으로 segmentation을 수행하고 물체의 크기/위치/포즈 변화를 시뮬레이션해야 한다. 
- **Control image**: 레이아웃이나 모양을 정규화하는 마스크, edge map, depth map이 포함된다. 

모델은 다양한 카테고리의 이미지에 대해 고유한 동작을 수행해야 한다. 따라서 학습 가능한 카테고리 임베딩을 설계하고 해당 이미지 토큰에 이미지 프롬프트를 더한다.

inference 과정에서는 기본 프롬프트를 기준으로 컨텍스트 프롬프트와 이미지 프롬프트를 자동으로 분석할 수 있다. 따라서 UniReal은 프롬프트 작성에 추가적인 노력을 필요로 하지 않는다. 하지만 사용자는 task 프롬프트와 이미지 프롬프트를 수동으로 수정하여 더욱 새로운 효과를 얻을 수 있다.

### 2. Dataset Construction
<center><img src='{{"/assets/img/unireal/unireal-fig3.webp" | relative_url}}' width="70%"></center>

##### 데이터 구축 파이프라인
먼저 캡션 모델을 사용하여 동영상의 캡션을 얻는다. 그런 다음, 편집 전/후 이미지로 두 프레임을 무작위로 선택하고 동영상 레벨의 캡션을 instruction으로 사용한다. 이러한 종류의 데이터를 **Video Frame2Frame**이라고 부르며, 이 데이터만으로도 기본적인 편집 기능을 갖춘 모델을 학습시킬 수 있다. 또한, GPT4o mini를 사용하여 20만 개의 고품질 샘플에 대해 두 프레임 간의 더욱 정확한 instruction을 얻었다.

또한, 그라운딩 캡션 모델인 [Kosmos-2](https://arxiv.org/abs/2306.14824)를 사용하여 해당 엔티티의 bounding box를 포함하는 이미지 캡션을 생성한다. 그런 다음, 한 프레임의 bounding box를 프롬프트로 사용하여 [SAM2](https://kimjy99.github.io/논문리뷰/segment-anything-2)가 두 프레임의 마스크 트랙을 가져오도록 한다. 이렇게 하여 두 동영상 프레임에 있는 여러 물체의 캡션과 마스크를 얻는다. 이러한 데이터는 이미지 커스터마이징 (**Video Multiobject**), 물체 삽입 (**Video Object Insertion**), 로컬 인페인팅 (**Video ObjectAdd**) 등을 지원할 수 있다. 

또한, [Kosmos-2](https://arxiv.org/abs/2306.14824)로 레이블링된 마스크와 캡션을 재사용하여 referring segmentation (**Video SEG**)을 지원한다. 이미지 인식 모델을 활용하여 depth map과 edge map을 추출하고, 제어 가능한 이미지 생성 및 이미지 인식 (**Video Control**)을 지원한다.

##### 학습 데이터 개요
<center><img src='{{"/assets/img/unireal/unireal-table1.webp" | relative_url}}' width="55%"></center>
<br>
학습에 사용된 데이터셋은 위 표와 같다. 구축된 동영상 기반 데이터셋 외에도 특정 task들을 위해 오픈소스 데이터를 사용하고, instruction 기반 이미지 편집 및 레퍼런스 기반 물체 삽입을 위해 자체 데이터셋을 사용하였다. 이러한 데이터셋을 컨텍스트 프롬프트와 통합하는 것이 중요하다. 

- 프레임 간의 optical flow와 픽셀 MSE를 분석하여 각 샘플에 "static/dynamic scenario"라고 컨텍스트 프롬프트를 부여한다.
- Instruction 기반 편집 데이터셋은 합성 스타일이므로 "synthetic style"로, 실제 이미지 데이터셋에는 "realistic style"이라고 컨텍스트 프롬프트를 부여한다.
- 동영상 물체 삽입의 경우 "with reference objects"와 같은 컨텍스트 프롬프트를 주고, 마스크나 depth map 등을 예측하도록 모델을 학습시키는 경우 "perception task"라고 컨텍스트 프롬프트를 부여한다.

## Experiments
- 학습 디테일
  - 파라미터 개수: 5B
  - 학습 단계
    - 먼저 text-to-image 및 text-to-video 데이터로 사전 학습시켜 256 해상도에서 기본적인 생성 능력을 확보
    - 모든 데이터셋으로 모델을 학습시켜 여러 이미지 생성/편집 task(256 해상도)를 수행
    - 해상도를 512와 1024로 점진적으로 높임
  - learning rate: $1 \times 10^{-5}$ (각 학습 단계마다 warm-up)
  - loss: [flow matching](https://arxiv.org/abs/2210.02747)을 따름
  - 이미지 패치에 위치 임베딩을 적용하고 학습 이미지의 종횡비가 서로 다르기 때문에, 다양한 크기와 종횡비를 처리할 수 있음

### 1. Comparisons with Existing Works
다음은 instruction 기반 이미지 편집에 대한 비교 결과이다. 

<center><img src='{{"/assets/img/unireal/unireal-fig4.webp" | relative_url}}' width="100%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/unireal/unireal-table2.webp" | relative_url}}' width="75%"></center>
<br>
다음은 이미지 커스터마이징에 대한 비교 결과이다. 

<center><img src='{{"/assets/img/unireal/unireal-fig5.webp" | relative_url}}' width="100%"></center>
<span style="display: block; margin: 1px 0;"></span>
<center><img src='{{"/assets/img/unireal/unireal-table3.webp" | relative_url}}' width="53%"></center>
<br>
다음은 물체 삽입에 대한 비교 결과이다. 

<center><img src='{{"/assets/img/unireal/unireal-fig7.webp" | relative_url}}' width="73%"></center>
<br>
다음은 user study 결과이다.

<center><img src='{{"/assets/img/unireal/unireal-fig6.webp" | relative_url}}' width="65%"></center>

### 2. Analysis for the Core Components
다음은 (위) 이미지 프롬프트와 (아래) 컨텍스트 프롬프트에 따른 다양한 결과이다. 

<center><img src='{{"/assets/img/unireal/unireal-fig8.webp" | relative_url}}' width="95%"></center>
<br>
다음은 학습 데이터에 대한 ablation study 결과이다. 

<center><img src='{{"/assets/img/unireal/unireal-fig9.webp" | relative_url}}' width="95%"></center>
<br>
다음은 구성 요소에 대한 ablation study 결과이다. 

<center><img src='{{"/assets/img/unireal/unireal-table4.webp" | relative_url}}' width="60%"></center>

### 3. More Applications
다음은 UniReal의 다양한 애플리케이션 예시들이다. 

<center><div style="overflow-x: auto; width: 100%;">
  <div style="width: 160%;">
    <img src='{{"/assets/img/unireal/unireal-fig10.webp" | relative_url}}' width="100%">
  </div>
</div></center>