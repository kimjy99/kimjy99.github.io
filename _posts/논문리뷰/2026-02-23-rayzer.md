---
title: "[논문리뷰] RayZer: A Self-supervised Large View Synthesis Model"
last_modified_at: 2026-02-23
categories:
  - 논문리뷰
tags:
  - Transformer
  - Novel View Synthesis
  - 3D Vision
  - ICCV
excerpt: "RayZer 논문 리뷰 (ICCV 2025)"
use_math: true
classes: wide
---

> ICCV 2025. [[Paper](https://arxiv.org/abs/2505.00702)] [[Page](https://hwjiang1510.github.io/RayZer/)] [[Github](https://github.com/hwjiang1510/RayZer)]  
> LHanwen Jiang, Hao Tan, Peng Wang, Haian Jin, Yue Zhao, Sai Bi, Kai Zhang, Fujun Luan, Kalyan Sunkavalli, Qixing Huang, Georgios Pavlakos  
> The University of Texas at Austin | Adobe Research | Cornell University  
> 1 May 2025  

<center><img src='{{"/assets/img/rayzer/rayzer-fig1.webp" | relative_url}}' width="100%"></center>

## Introduction
본 논문에서는 self-supervision으로 학습되고 새로운 3D awareness를 보여주는 대규모 멀티뷰 3D 모델인 **RayZer**를 소개한다. RayZer의 입력은 연속적인 동영상 프레임 또는 순서가 지정되지 않은 멀티뷰 캡처에서 샘플링된 포즈 및 calibration 정보가 없는 멀티뷰 이미지이다. RayZer는 먼저 카메라 파라미터를 복구하고, 장면 표현을 재구성한 다음, 새로운 시점을 렌더링한다. Self-supervised learning의 핵심 아이디어는 렌더링에 GT 포즈를 사용하는 대신, RayZer 자체가 예측한 카메라 포즈를 사용하여 photometric 정보를 제공하는 시점을 렌더링하는 것이다. 따라서 RayZer는 3D geometry나 카메라 포즈에 대한 정보 없이 학습될 수 있다. Inference 과정는 장면별 최적화 없이 feed-forward 방식으로 카메라 및 장면 표현을 예측한다.

RayZer는 자체적으로 예측한 카메라 포즈를 학습에 사용하기 때문에, 이 self-supervised task는 3D-aware 이미지 오토인코딩으로 해석될 수 있다. RayZer는 모든 이미지를 두 부분으로 나눈다. 한 부분은 장면 표현을 예측하는 입력 뷰이고, 다른 부분은 self-supervision을 제공하는 타겟 뷰이다. 이는 두 번째 부분 집합의 추정된 포즈를 사용하여 첫 번째 부분 집합에서 예측된 장면 표현을 렌더링함으로써 달성되며, 3D-aware하지 않은 단순한 솔루션을 방지한다.

RayZer는 self-supervised learning을 용이하게 하기 위해 3D 표현, 렌더링 방정식, 3D 정보를 활용한 아키텍처 없이 오직 transformer만을 사용하여 구축되었다. 이러한 설계는 다른 모달리티 분야의 대규모 self-supervised 모델에서 영감을 받았으며, RayZer가 도메인 지식을 유연하고 효과적으로 학습할 수 있도록 한다. RayZer에 통합된 유일한 3D prior는 카메라, 픽셀(이미지), 장면 간의 관계를 동시에 모델링하는 ray 구조이다. 구체적으로, RayZer는 먼저 카메라 포즈를 예측한 다음, 이를 픽셀 정렬된 Plücker ray map으로 변환하여 장면 재구성을 가이드한다. 이러한 ray 기반 표현은 구조와 모션 사이의 chicken-and-egg problem을 해결하는 강력한 prior 역할을 하며, 학습 과정에서 카메라와 장면 표현이 서로를 정규화할 수 있도록 한다.

## Method
### 1. RayZer’s Self-supervised Learning
본 논문에서는 정적 장면 모델링의 표준 설정에 초점을 맞추었다. RayZer의 입력은 calibration되지 않은 이미지 집합

$$
\begin{equation}
\mathcal{I} = \{I_i \in \mathbb{R}^{H \times W \times 3} \, \vert \, i = 1, \ldots, K\}
\end{equation}
$$

이다. 출력은 입력의 parametrization, 즉 카메라 intrinsics, 시점별 카메라 포즈, 장면 표현으로, novel view synthesis (NVS)를 가능하게 한다. 저자들은 이러한 표현을 예측하기 위해 RayZer 모델을 구축하고 self-supervised learning 방식으로 학습시켰다. 즉, 학습 과정에서 3D supervision, 즉 3D geometry나 GT 카메라 포즈를 사용하지 않는다.

RayZer를 self-supervision으로 학습시키기 위해 데이터 정보 흐름을 제어한다. 입력 이미지 $\mathcal{I}$를 겹치지 않는 두 개의 부분집합 $$\mathcal{I}_\mathcal{A}$$와 $$\mathcal{I}_\mathcal{B}$$로 분할한다. RayZer는 $$\mathcal{I}_\mathcal{A}$$를 사용하여 장면 표현을 예측하고, $$\mathcal{I}_\mathcal{B}$$를 supervision을 제공하는 데에 사용한다. 따라서 RayZer는 $$\mathcal{I}_\mathcal{B}$$에 해당하는 이미지 $$\hat{\mathcal{I}}_\mathcal{B}$$를 렌더링하고, photometric loss를 적용한다.

$$
\begin{equation}
\mathcal{L} = \frac{1}{\vert \mathcal{I}_\mathcal{B} \vert} \sum_{\hat{I} \in \hat{\mathcal{I}}_\mathcal{B}} (\textrm{MSE}(I, \hat{I}) + \lambda \cdot \textrm{Percep} (I, \hat{I}))
\end{equation}
$$

두 데이터셋은 학습 과정에서 무작위로 샘플링된다.

### 2. RayZer Model
<center><img src='{{"/assets/img/rayzer/rayzer-fig3.webp" | relative_url}}' width="100%"></center>

##### Overview
RayZer는 포즈를 모르고 calibration되지 않은 입력 이미지로부터 카메라 파라미터와 장면 표현을 모두 복원한다. RayZer의 핵심 설계 요소는 카메라 및 장면 표현을 **단계적**으로 예측하는 방식이다. 이는 노이즈가 있는 카메라조차도 더 나은 장면 재구성을 위한 강력한 조건이 될 수 있다는 사실에 기반하며, 이는 기존의 SfM 방식과 유사하고 최근의 reconstruction-first 방식과는 대조적이다. 이러한 설계는 학습 과정에서 포즈와 장면 예측에 대한 상호 정규화를 제공하여 self-supervised learning을 촉진한다.

RayZer는 확장성과 유연성을 활용하는 순수 transformer 기반 모델을 사용한다. RayZer는 먼저 입력 이미지를 tokenize하고 transformer 기반 인코더를 사용하여 모든 뷰의 카메라 파라미터를 예측한다. 이 단계에서 카메라는 intrinsic과 SE(3) 카메라 포즈로 표현된다. 이러한 저차원의 기하학적으로 잘 정의된 parametrization은 이미지 정보를 카메라 표현에서 분리하는 데 도움이 된다.

RayZer는 SE(3) 카메라 포즈와 내부 정보를 Plücker ray map으로 변환하여 예측된 카메라를 픽셀 정렬된 ray들로 표현한다. 이 ray 기반 표현은 2D ray-픽셀 정렬과 3D ray geometry를 모두 캡처하여 카메라 모델의 물리적 속성을 캡슐화하는 세밀한 ray 레벨 디테일을 제공한다. Ray map은 뒤따르는 재구성 단계를 개선하기 위한 조건으로 사용된다.

RayZer는 이미지와 $$\mathcal{I}_\mathcal{A}$$의 예측된 Plücker ray로부터 또 다른 transformer 기반 인코더를 사용하여 latent 장면 표현을 예측한다. 그런 다음 RayZer는 이전에 추정된 $$\mathcal{I}_\mathcal{B}$$의 카메라를 사용하여 $$\hat{\mathcal{I}}_\mathcal{B}$$를 예측하고, 이를 통해 self-supervision을 제공한다.

##### Image Tokenization
모든 $K$개의 입력 이미지 $\mathcal{I}$에 대해, ViT를 따라 겹치지 않는 패치로 분할한다. 각 패치는 $\mathbb{R}^{s \times s \times 3}$ 크기이며, 여기서 $s$는 패치 크기이다. Linear layer를 사용하여 각 패치를 $d$차원의 토큰으로 인코딩하고, 각 이미지에 대해 token map $$f_i \in \mathbb{R}^{h \times w \times d}$$를 생성한다 ($h = H/s$, $w = W/s$).

다음으로 토큰에 위치 임베딩을 추가하여 후속 모델이 각 토큰의 공간적 위치와 해당 이미지 인덱스를 인식할 수 있도록 한다. 구체적으로, linear layer를 사용하여 공간 위치 임베딩과 이미지 인덱스 위치 임베딩을 결합한다. 이미지 인덱스 위치 임베딩은 동일한 이미지의 모든 토큰에서 공유된다. 연속적인 동영상 프레임으로 학습할 경우, 이러한 이미지 인덱스 임베딩은 순차적인 정보도 인코딩하여 포즈 추정에 도움이 된다. 마지막으로, 모든 이미지의 token map을 $$\textbf{f} \in \mathbb{R}^{Khw \times d}$$로 reshape한다.

##### Camera Estimator
카메라 추정 모델 $$\mathcal{E}_\textrm{cam}$$은 모든 입력 이미지에 대해 카메라 파라미터, 즉 카메라 포즈와 intrinsic을 예측한다. 모든 시점에 대한 예측을 위해 초기 feature로 $\mathbb{R}^{1 \times d}$ 크기의 학습 가능한 카메라 토큰을 사용한다. 이 토큰을 $K$번 반복하여 $$\textbf{p} \in \mathbb{R}^{K \times d}$$를 만들어 모든 입력 이미지에 대응할 수 있도록 한다. 그런 다음, self-attention transformer layer로 구성된 카메라 추정 모델을 사용하여 다음과 같이 카메라 토큰을 업데이트한다.

$$
\begin{equation}
\{\textbf{f}^\ast, \textbf{p}^\ast\} = \mathcal{E}_\textrm{cam} (\{\textbf{f}, \textbf{p}\})
\end{equation}
$$

($$\{\cdot, \cdot\}$$은 토큰 차원을 대한 concat)

$$\textbf{f}^\ast$$는 다음 계산에는 사용되지 않고, transformer layer에서 $\textbf{p}$를 업데이트하기 위한 컨텍스트로만 사용된다. 구체적으로 transformer layer들은 다음과 같이 작동한다.

$$
\begin{aligned}
& \textbf{y}^0 = \{\textbf{f}, \textbf{p}\} \\
& \textbf{y}^l = \textrm{TransformerLayer}^l (\textbf{y}^{l-1}), \quad l = 1, \ldots, l_T \\
& \{\textbf{f}^\ast, \textbf{p}^\ast\} = \textrm{split} (\textbf{y}^{l_T})
\end{aligned}
$$

($l_T$는 layer 수)

그런 다음 각 이미지에 대해 카메라 파라미터를 독립적으로 예측한다. 카메라 포즈 예측의 경우, 모호성을 해결하기 위해 상대적 카메라 포즈를 사용한다. 하나의 뷰를 레퍼런스로 선택하고, 다른 모든 뷰에 대해 레퍼런스 뷰에 대한 상대적 포즈를 예측한다. SO(3) rotation을 6D 표현을 사용하여 parametrize하고, 다음과 같이 2-layer MLP를 사용하여 상대적 포즈를 예측한다.

$$
\begin{equation}
p_i = \textrm{MLP}_\textrm{pose} ([\textbf{p}_i^\ast, \textbf{p}_c^\ast])
\end{equation}
$$

($[\cdot, \cdot]$는 feature 차원에 대한 concat, $$\textbf{p}_i^\ast$$와 $$\textbf{p}_c^\ast$$는 각각 이미지 $I_i$와 canonical view에 대한 카메라 토큰)

출력 $$p_i \in \mathbb{R}^9$$는 이미지 $I_i$에 대한 SE(3) 포즈 $$\textbf{P}_i$$로 변환된다.

Intrinsic 예측의 경우, 다음과 같은 가정 하에 하나의 초점 거리 값을 사용하여 intrinsic을 parametrize한다.

1. x축과 y축의 초점 거리가 같다.
2. 모든 시점이 동일한 intrinsic을 공유하고 있다.
3. Principal point가 이미지 중심에 있다.

2-layer MLP를 사용하여 초점 거리를 예측한다.

$$
\begin{equation}
\textrm{focal} = \textrm{MLP}_\textrm{focal} (\textbf{p}_c^\ast)
\end{equation}
$$

예측된 초점 거리는 intrinsic 행렬 $$\textbf{K} \in \mathbb{R}^{3 \times 3}$$으로 변환된다.

##### Scene Reconstructor
이미지 세트 $$\mathcal{I}_\mathcal{A}$$로부터 장면 표현을 예측하고, 추가적으로 이전에 예측된 카메라 파라미터 $$\mathcal{P}_\mathcal{A} = \{ (\textbf{P}_i, \textbf{K}) \vert I_i \in \mathcal{I}_\mathcal{A} \}$$를 조건으로 한다. 먼저 $$\mathcal{P}_\mathcal{A}$$를 각 이미지에 대해 픽셀 정렬된 Plücker ray $$\mathcal{R} \in \mathbb{R}^{K \times H \times W \times 6}$$으로 변환한다. 이미지와 유사하게 $\mathcal{R}$도 linear layer를 사용하여 패치 수준의 토큰 $$\textbf{r} \in \mathbb{R}^{Khw \times d}$$로 tokenize한다. $$\mathcal{I}_\mathcal{A}$$에 해당하는 이미지 토큰 $$\textbf{f}_\mathcal{A}$$와 Plücker ray 토큰 $$\textbf{r}_\mathcal{A}$$를 feature 차원을 따라 2-layer MLP로 융합한다.

$$
\begin{equation}
\textbf{x}_\mathcal{A} = \textrm{MLP}_\textrm{fuse}([\textbf{f}_\mathcal{A}, \textbf{r}_\mathcal{A}])
\end{equation}
$$

중요한 것은, 이 융합에 transformer 출력 $$\textbf{f}^\ast$$ 대신 원본 이미지 토큰 $\textbf{f}$를 사용한다는 점이다. 카메라 추정 transformer가 $$\mathcal{I}_\mathcal{B}$$의 토큰을 포함하는 글로벌 컨텍스트에 접근할 수 있기 때문에 이러한 설계 선택은 $$\mathcal{I}_\mathcal{B}$$에서 정보가 유출되는 것을 방지한다.

다음으로, self-attention transformer layer로 구성된 장면 재구성 모델 $$\mathcal{E}_\textrm{scene}$$을 사용하여 latent 장면 표현을 예측한다. 이 표현을 초기화하기 위해 학습 가능한 토큰 집합 $$\textbf{z} \in \mathbb{R}^{L \times d}$$를 사용하는데, 여기서 $L$은 토큰의 개수이다.

$$
\begin{equation}
\{\textbf{z}^\ast, \textbf{x}_\mathcal{A}^\ast\} = \mathcal{E}_\textrm{scene} (\{\textbf{z}, \textbf{x}_\mathcal{A}\})
\end{equation}
$$

업데이트 규칙은 $$\mathcal{E}_\textrm{cam}$$과 동일하다. $$\textbf{z}^\ast$$는 $$\mathcal{I}_\mathcal{A}$$에서 예측된 최종 latent 장면 표현이며, $$\textbf{x}_\mathcal{A}^\ast\$$는 버려진다.

##### Rendering Decoder
[LVSM](https://kimjy99.github.io/논문리뷰/lvsm)을 따라 transformer 기반 디코더를 사용하여 렌더링한다. 먼저 렌더링하고자 하는 포즈를 픽셀 정렬된 Plücker ray로 표현하고, linear layer를 사용하여 Plücker ray를 tokenize하여 타겟 토큰 $\textbf{r} \in \mathbb{R}^{hw \times d}$를 얻는다. 다음으로, transformer layer로 구성된 디코더 $$\mathcal{D}_\textrm{render}$$를 사용하여 토큰을 업데이트함으로써 장면 정보를 융합한다.

$$
\begin{equation}
\{ \textbf{r}^\ast, \textbf{z}^\prime \} = \mathcal{D}_\textrm{render} (\{ \textbf{r}, \textbf{z}^\ast \})
\end{equation}
$$

여기서 $$\textbf{z}^\prime$$는 이후에 버려지며, $$\mathcal{D}_\textrm{render}$$의 업데이트 규칙은 이전에 소개된 모듈과 동일하다. 마지막으로, MLP를 사용하여 패치 수준에서 RGB 값을 디코딩한다.

$$
\begin{equation}
\hat{I} = \textrm{MLP}_\textrm{rgb} (\textbf{r}^\ast) \in \mathbb{R}^{hw \times (3s^2)}
\end{equation}
$$

$$\hat{I}$$를 reshape하여 2D 공간 구조를 복원하고 최종 렌더링된 이미지를 $\mathbb{R}^{H \times W \times 3}$로 생성한다. 

학습 과정에서 $$\hat{\mathcal{I}}_\mathcal{B}$$에 해당하는 예측된 Plücker ray map $$\mathcal{R}_\mathcal{B}$$를 사용하여 $$\hat{\mathcal{I}}_\mathcal{B}$$의 이미지를 렌더링하고 self-supervised loss를 계산한다.

## Experiments
- 데이터셋: DL3DV, RealEstate, Objaverse
  - 입력 뷰 개수 ($$\mathcal{I}_\mathcal{A}$$): 16, 5, 12
  - 타겟 뷰 개수 ($$\mathcal{I}_\mathcal{B}$$): 8, 5, 8

### 1. Results
다음은 DL3DV에서의 평가 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-table1.webp" | relative_url}}' width="75%"></center>
<br>
다음은 RealEstate에서의 평가 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-table2.webp" | relative_url}}' width="75%"></center>
<br>
다음은 Objaverse에서의 평가 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-table3.webp" | relative_url}}' width="75%"></center>
<br>
다음은 RealEstate와 DL3DV에서의 시각화 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-fig4.webp" | relative_url}}' width="100%"></center>
<br>
다음은 Objaverse에서의 시각화 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-fig5.webp" | relative_url}}' width="70%"></center>
<br>
다음은 연속적인 입력과 무작위 입력에 대한 비교 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-table4.webp" | relative_url}}' width="75%"></center>

### 2. Analysis of Camera Poses
다음은 예측된 카메라 포즈를 시각화한 예시들이다.

<center><img src='{{"/assets/img/rayzer/rayzer-fig6.webp" | relative_url}}' width="70%"></center>
<br>
다음은 입력 뷰에 대해 RayZer가 예측한 포즈를 interpolation한 후 렌더링한 결과이다. (interpolation 계수는 GT 포즈를 기준으로 계산)

<center><img src='{{"/assets/img/rayzer/rayzer-table5.webp" | relative_url}}' width="76%"></center>
<br>
다음은 포즈 인코더 학습 방법에 따른 성능을 비교한 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-table6.webp" | relative_url}}' width="66%"></center>

### 3. Ablation Study
다음은 ablation study 결과이다.

<center><img src='{{"/assets/img/rayzer/rayzer-table7.webp" | relative_url}}' width="72%"></center>